\chapter{Design Space Exploration}\label{sec:designspace}
In this chapter, we will explore possible design decisions with the goal of guaranteeing efficient in-place updates. First, we will evaluate linear type theory, quantitative type theory and uniqueness type theory, and argue why we decide for the latter. Second, we will discuss the dimensions in the design space of uniqueness type systems.

\section{Substructural Framework}
Let us first quickly repeat some of the important points from \cref{sec:ltt}, \cref{sec:qtt} and \cref{sec:uniqueness} that are relevant to this section:
\begin{enumerate}
	\item Properly linear types make a guarantee for the usage of a variable in the future, but not the past
	\item Uniqueness type theory makes a guarantee for the usage of a variable in the past, but not the future
	\item Invariably unique types are always unique and can never discard their uniqueness
	\item Properly linear types can make a guarantee for the past on particular types if all constructors of the type return linear values
	\item Properly linear types can discard the linearity of particular types with special library functions, but are not capable of doing so in constant time for nested linear types
	\item Quantitative type theory introduces an ``erasure'' quantity to specify computationally relevant terms in dependent types and count only those in the substructural portion of the type system
	\item Quantitative type theory inherits the properties of proper linearity
	\item Both quantitative- and properly linear type theories can use linear functions that return values of linear type to produce values of non-linear type by demanding inputs of non-linear type to the function
	\item Allowing the use of linear functions to produce non-linear outputs forces constructors of unique types to be formulated in continuation-passing style
\end{enumerate}

\subsubsection{Properly linear types}
Because of 1.\ and despite 4.\ and 5., we feel that properly linear types are simply not the right tool for the job of ensuring uniqueness for destructive updates. 

Although they can be made to ensure uniqueness with careful library design, the tricks involved often seem to go against the grain of the initial decision to allow for a coercion from non-linear to linear types and are never without sizeable limitations, like the discarding of linearity only being available for a couple of types and nesting of such unique types being complicated. 

While \cite{spiwack_linearly_2022} manage to remove some of these limitations, they also add an entirely separate system of linear capabilities on top of the linear type system.

\subsubsection{Invariably unique types}
Meanwhile, due to 3., we think that invariably unique types are too restrictive for safe destructive updates. 

In all linear and uniqueness type systems known to us, there are significant limitations in expressivity, where it can occasionally happen that the type system cannot ensure the uniqueness of a value that is indeed referenced uniquely at runtime. Some of these limitations can be mitigated by choosing a different program encoding, while in other cases it is extremely difficult to do so. 

Ideally, as we are only concerned with efficient updates, we would like our type system to be fairly non-invasive, so as not to bother users. Hence, since the reference counting system described in \cref{sec:beans} is already suitable to implement safe destructive updates at runtime, we feel that it is absolutely crucial that users can fall back to this manual type-less mode of ensuring uniqueness if they cannot get the type system to guarantee the invariants that they need. This is especially pressing because we intend to add a substructural type system to the existing eco-system of Lean, where none of the code is annotated with linearity or uniqueness annotations. As per 2., uniqueness type theory has the desired property.

\subsubsection{Term language}
Finally, we must decide what the term language of our type system will be. 

Due to 7., 8.\ and 9., existing formulations of quantitative type theory are not readily suited for uniqueness. Graded modal dependent type theory \citep{moon_graded_2021} is still an active area of research and implementing something along these lines would be far beyond the scope of this thesis. Transferring the core idea of QTT from 6.\ and removing all the components that make QTT ``quantitative'' may be possible, but would both increase the complexity of the resulting system due to the extra erasure attribute and require additional adjustments to the existing compiler toolchain to respect erasure. 

Because of this, we decide against combining dependent type theory with uniqueness type theory and instead focus on a derivate of the LCNF language described in \cref{sec:irs} in the later stages of the compiler toolchain when type dependencies have been erased. Type erasure will lose us some analysis precision, but in turn integrating the resulting type system with the Lean compiler toolchain should be much easier.

\section{Uniqueness Type Systems}
In \cref{sec:uniqueness} and \cref{sec:borrowing}, we describe two key challenges that come up when designing a uniqueness type system. The first is that non-unique values, including closures of non-unique functions, cannot be allowed to contain unique values. The second is that functions which use an argument in a read-only manner still consume it, thus losing the reference in the process.

\subsection{Higher-Order Functions}\label{sec:hof}
As discussed in \cref{sec:uniqueness}, invariably unique type systems ensure that shared containers cannot contain unique values during the construction of a value, whereas Clean ensures it during the deconstruction of a value by disallowing the deconstruction of a shared value if it contains unique values, as types can lose their uniqueness. Higher-order functions complicate this matter because function types typically do not reveal the types of the values in their closure and because implementations usually commit to one particular function pointer when the higher order function is created. 

To see why this is an issue, consider a function $f : *(*\alpha \to *(!\beta \to *\alpha))$, using the notation introduced in \cref{sec:uniqueness} where $*$ represents a unique type and $!$ represents a non-unique type. There are two ways in which the uniqueness of the first argument can be leveraged: In the construction of the return value of type $*\alpha$, and in the resulting code for the function $f$ where the first argument may get updated destructively. If we partially apply the first argument, the type becomes $f\ a : *(!\beta \to *\alpha))$ and the information that the first argument was unique is lost, despite $a$ being in the closure of $f\ a$. Now, if we discard the uniqueness of $f$ and apply it twice, then the return type may incorrectly suggest that the return value is unique, and we may even accidentally destructively update the first argument to the function, despite the fact that it is shared between the two function invocations.

There are several possible solutions to this problem, most of which have previously been covered by \cite{de_vries_making_2009}. 

\subsubsection{``Necessarily unique''}
The first is to take the approach that Clean uses and disallow discarding the uniqueness of function types altogether. Clean ensures this by introducing another kind of type into its type system, so called ``necessarily unique'' types. In practice, ``necessarily unique'' is the same thing that we have been calling ``invariably unique'' so far: The value is unique when it has been constructed and can never lose its uniqueness. Unfortunately, having support for ``necessarily unique'' values only in functions creates two additional problems.

First, in a type system with polymorphic types, the fact that functions in particular can be invariably unique also precludes type variables from discarding their uniqueness, as they could be substituted for functions.

And second, if an invariably unique function is stored in a unique container that then discards its uniqueness, we must make the function unavailable during the deconstruction of the value, lest it could have been shared. As argued in \cref{sec:uniqueness}, for other unique values in non-unique containers, we do not have to be this restrictive and could instead also discard the uniqueness of the contained unique values during deconstruction.

Furthermore, a perspective put forward by \cite{sergey_linearity_2022} is that Clean's ``necessarily unique'' is too restrictive: For functions with unique values in their closure, we typically do not care about the uniqueness of the function itself, just that the function does not duplicate values in its closure. Hence, functions could really be properly linear, not invariably unique, allowing for some greater flexibility when joining two code paths where one yields a function with a unique value in its closure, whereas the other does not.

\subsubsection{Closure typing}
The second solution is what de Vries calls ``closure typing'' - instead of disallowing the discarding of uniqueness of a function altogether, an additional attribute is added to every function type that denotes whether the function contains a unique value in its closure. Then, when applying the function, the information of whether the closure contains a unique value is not lost, and applying a shared function with a unique value in its closure becomes disallowed.

\subsubsection{Higher-rank polymorphism}
The third solution outlined by de Vries is to attempt to do away with the coercion altogether and replace it with careful library design, though of another nature than the freeze function in Linear Haskell. 

First, de Vries argues that constructors of functions should leverage polymorphism in order to create values of polymorphic kind, e.g.\ $\mathrm{mkArray} :\ !\mathbb{N} \to *\mathrm{Array}\ \alpha$ becomes $\mathrm{mkArray} : \forall u.\ !\mathbb{N} \to u(\mathrm{Array}\ \alpha)$, where $u \in \{*, !\}$. However, this is not equivalent to the approach with the coercion, as we must commit to a concrete $u$ when constructing the array, and two code paths cannot use the same array in two separate ways anymore. 

To mitigate this, de Vries suggests using higher-rank types so that the constructor is typed as $\mathrm{mkArray} : \ !\mathbb{N} \to (\forall u.\ u(\mathrm{Array}\ \alpha))$ and two different code paths can instantiate $(\forall u.\ u(\mathrm{Array}\ \alpha))$ in two separate ways. Unfortunately, we feel that this is still not equivalent to being able to discard uniqueness: Two code paths can now instantiate the array in two separate ways, but after they instantiate it as needed, they cannot be joined anymore, as the concrete instantiations $*\mathrm{Array}\ \alpha$ and $!\mathrm{Array}\ \alpha$ are not compatible.

\subsubsection{Deleveraging uniqueness}
Finally, the fourth solution outlined by de Vries, originally due to \cite{harrington_uniqueness_2006}, is to allow the application of shared functions with a unique value in their closure, but to degrade the return type of the function to non-unique instead. 

As de Vries correctly points out, if this idea is used in a programming language, then ways of leveraging the uniqueness of the function argument other than the uniqueness of the return value must be addressed as well, e.g.\ the presence of destructive updates in the function. For destructive updates in particular, when creating a higher-order function, one possible implementation might yield two function pointers for the higher-order function: one for if the function remains unique in which destructive updates are used, and another for if the function becomes non-unique in which no destructive updates are used. Then, when the unique function is forced to discard its uniqueness, the function pointer with destructive updates can be swapped out for a function pointer without destructive updates, and the now violated uniqueness of the function argument cannot be leveraged anymore.

Since Clean uses uniqueness not just for destructive updates, but for threading I/O as well, de Vries argues that the approach is inadequate, as side-effecting functions like $\mathrm{closeFile} : *\mathrm{File} \to\ !\mathbb{B}$ cannot be prevented from leveraging the uniqueness of the $*\mathrm{File}$ argument, and closing a file twice will always be an error. However, we feel that uniqueness types are simply the wrong tool to handle I/O, as it is never desirable to make I/O values shared, and that Clean should instead use invariably unique types for I/O. Uniqueness types are better suited for situations in which the sharedness of a value is still admissible, like destructive updates in memory, where we can always copy a value if it is shared.

\subsubsection{Conclusion}
We feel that the first approach is overkill. If one introduces linearity into a uniqueness type system for functions, then one should also introduce linearity for all other types, so that linear functions can be nested in linear structures. This is essentially the approach of \cite{sergey_linearity_2022}, though as stated in \cref{sec:uniqueness}, their type system does not support nesting of unique types. This creates lots of extra work for users, and it seems excessive if all we want to do is guarantee safe destructive updates and handle higher-order functions correctly.

The second approach of closure typing also adds plenty of notational overhead to the type theory, as functions now have two separate type annotations, and the idea of not being able to apply a function that is present in the context seems unintuitive to us.

We think that the third approach introduces a lot of complexity in requiring higher-rank types to work, and even then, it does not fully replace the notion of discarding uniqueness.

Despite de Vries' criticism of it, we feel that the last approach is the most viable if all we care about are destructive updates of values in memory. However, this approach would require a hefty change to the runtime of Lean, as every unique higher-order function needs to carry two function pointers around, so that we can switch to the one without destructive updates when the function becomes shared. Because of this, we decided not to implement this approach yet, and will instead require all higher-order functions and the types within them to be non-unique. As this precludes us from making guarantees for monadic code, type classes and idioms using higher-order functions like the one in \cref{sec:beans}, this is a considerable limitation that we hope to resolve in the future.

\subsection{Implicit Coercion}\label{sec:coercions}
In most descriptions of linear and uniqueness type theory discussed so far, the coercion between non-linear/non-unique and linear/unique types has usually been implicit, e.g.\ passing a unique value to a shared parameter works, but will discard the uniqueness in the process. For linear type theories this is not an issue, as the coercion from non-linear types to linear types adds structure. However, for uniqueness type theories, the coercion from unique to shared discards the guarantee that the value is unique, and so the user may not want it to happen implicitly. 

Additionally, there is a question of when unique types are implicitly coerced. In Clean, this is only possible at function boundaries, and if one wishes to share a variable, then one must choose the respective parameter type accordingly. An explicit coercion operator would give users greater control over when coercion happens.

While the type theory that we will describe in \cref{sec:theory} also uses an implicit coercion, we think it is best to make it explicit when integrating the type theory with Lean 4.

\subsection{Uniqueness Propagation}
As discussed in \cref{sec:hof}, shared containers cannot be allowed to contain unique values. We have already explored possible approaches for how to ensure this for higher-order functions, but we are still lacking a more general framework for other types.

\subsubsection{Subtyping}
In Clean, a shared product is allowed to contain unique values, but upon deconstruction or projection, the type system checks that the projected values cannot be unique if the outer value is shared. In other words, if we discard the uniqueness of a product, then we cannot access its fields anymore. This is rather limiting, because we could instead also discard the uniqueness of the fields when we attempt to access them. 

Relatedly, there is a question of how deep the subtyping relation induced by the coercion from unique to non-unique types is. In Clean, it is very shallow and only allows discarding the uniqueness of the outer layer, i.e.\ we cannot pass a value of type $*(*\alpha \times *\beta)$ to a parameter of type $!(!\alpha\ \times\ !\beta)$, only a parameter of type $!(*\alpha \times *\beta)$, the fields of which cannot be accessed anymore. Similarly, passing a value of type $*(*\alpha \times *\beta)$ to a parameter of type $*(!\alpha \times *\beta)$ is not possible either, though doing so is sound. A less shallow subtyping relation would resolve these issues.

If we make the subtyping relation less shallow, then coercions should also propagate the change in the outer attribute to the values contained within, so that $!(*\alpha \times *\beta)$ and $!(!\alpha\ \times\ !\beta)$ are not differently annotated but equivalent types, and $!(*\alpha \times *\beta)$ becomes unrepresentable. For types like $*(*\alpha \times *\beta)$ where the attributes are floated out, this is straight-forward, as we can propagate the outer sharedness annotation to the inner components of the type.

\subsubsection{Algebraic data types}
For algebraic data types (ADTs), this is not as straight-forward, as there is a question of what to do with the attributes associated with fields of the ADT. One possibility would be to float out every attribute within the ADT, so that fields use an attribute variable $m$ to refer to a concrete attribute passed to the ADT, after which $!$ can be propagated directly in the arguments of the ADT, much like in $*(*\alpha \times *\beta)$. However, for large ADTs, this will very quickly accumulate dozens of attribute arguments, leading to a huge notational overhead.

Instead, for our type system described in \cref{sec:theory}, we decide that fields with a $*$ attribute are ``unique if the outer value is unique''. In other words, for the uniqueness annotations within ADTs, sharedness is not propagated directly, only when deconstructing the value and accessing the fields. Since these attributes are not floated out, delaying the propagation does not make us end up with differently annotated types that are equivalent, like it would be the case for $*(*\alpha \times *\beta)$.

\subsection{Borrowing}
As discussed in \cref{sec:borrowingbackground}, most implementations of borrowing use a form of type-driven escape analysis. We think that introducing extra attributes like observer-types into the type system that users need to keep accurate track of in order to be able to access borrowing is too much of an annotational burden.

Instead, we will make use of the fact that escape analyses are very local in nature; whether a variable escapes or not can be decided by following the data flow of the variable from the start of the function to the return value. Hence, we implement a data flow analysis \cite{allen_program_1976} in \cref{sec:borrowing}. Due to the inherent locality of the analysis, we will not need any type information to run it.

While we will not touch on it further, it is worth pointing out that adding extra annotations describing whether a function parameter is borrowed may still be a good idea for maintenance purposes, despite the fact that we can compute this information. When integrating our type system with Lean 4, it may hence be a good idea to add these annotations as well.