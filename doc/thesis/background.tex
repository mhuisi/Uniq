\chapter{Background}\label{sec:background}

\section{Lean 4 Theorem Prover}\label{sec:lean4}
Lean 4 is a proof assistant and programming language developed primarily by Leonardo de Moura at Microsoft Research and Sebastian Ullrich at KIT with numerous open source contributions by many other authors.

Up to and including version 3, Lean served only as a proof assistant, i.e.\ an interactive tool where users can input proofs that are then checked by the proof assistant. Many proof assistants also implement proof-generating automation, so-called ``tactic languages'', to make the task of writing a perfectly formal proof by hand less tedious. In Lean 3, the previous version of Lean, the same term language was used for proofs, theorem statements, definitions, programs, type declarations, specifications, and implementing automation. We will go into some detail on how Lean uses a single unified language for all of these things in \cref{sec:dtt}. For automation, the term language was also evaluated by a separate interpreter for more efficient execution.

Unfortunately, evaluating the term language using a separate interpreter would still yield inadequate performance, both for implementing more demanding automation and for implementing real world programs, which meant that such demanding programs were written in C++ instead and then made available to Lean using a foreign-function interface (FFI) \citep{ullrich_counting_2020}.

To improve on this, Lean 4 now implements its own self-hosted compiler toolchain for both a C backend and a work-in-progress LLVM \citep{lattner_llvm_2004} backend, including its own IR, optimization pipeline and custom garbage collection algorithm. Being almost entirely self-hosted, Lean 4 is now also well capable of being used as a general purpose programming language.

\subsubsection{Examples}
Let us now consider some basic examples of Lean 4 code to get a feeling for the language. It should be noted that all of the following can be expressed more succinctly, but that we have chosen not to do so in order to make these snippets easier to grasp for readers not familiar with Lean.\\
\begin{code}
def List.map (f : α → β) : List α → List β
  | []      => []
  | x :: xs => f x :: map f xs
\end{code}

\lstinline|List.map| is implemented by recursion on the second argument. The empty list again yields the empty list. If the list is a cons cell, we map the head of the cons cell using \lstinline|f|, recurse on the tail and then build a new cons cell from the results of both.\\
\begin{code}
def List.get? : List α → Nat → Option α
  | [],      _     => Option.none
  | x :: _,  0     => Option.some x
  | _ :: xs, n + 1 => List.get? xs n
\end{code}

\lstinline|List.get?| is implemented by recursion both on the provided list and the index provided in the second argument. It yields an \lstinline|Option α|, i.e.\ either \lstinline|Option.some α| if the index is in the list, or \lstinline|Option.none| otherwise.\\
\begin{code}
def Array.groupBy (p : α → α → Ordering) (xs : Array α)
  : RBMap α (Array α) p := Id.run do
  let mut result : RBMap α (Array α) p := RBMap.empty
  for x in xs do
    let group := Option.getD (RBMap.find? p result x) #[]
    result := RBMap.insert p result x (Array.push group x)
  return result
\end{code}

\lstinline|Array.groupBy| is implemented using an imperative domain-specific language (DSL) based on do-notation \citep{ullrich_beyond_2022}. It takes a relation \lstinline|p| that yields an \lstinline|Ordering|, i.e.\ whether the first argument is greater than, smaller than or equal to the second argument, as well as the array to group the elements of. It returns a red-black map ordered by \lstinline|p|, where the keys are arbitrary representatives of the group and the values denote groups \lstinline|Array α| of \lstinline|p|-equivalent values.

In order to use do-notation, we need to run the code in a monad. Since we do not intend to accumulate any effects and only use do-notation for its imperative domain-specific language (DSL), we use the \lstinline|Id| monad, entering it using \lstinline|Id.run|. First, we initialize a mutable but empty red-black map, denoting our result. Then, we iterate over every element \lstinline|x| in the provided array and look for a group in the current result that is \lstinline|p|-equivalent to \lstinline|x|. If we find such a group, we store it in \lstinline|group|. Otherwise, we allocate a new group for elements \lstinline|p|-equivalent to \lstinline|x| using the call to \lstinline|Option.getD|, which returns the first element if it was equal to \lstinline|some x| and otherwise returns the second element if it was equal to \lstinline|none|. Then, we add \lstinline|x| itself to the group and re-insert it into our mutable \lstinline|result| map. At the end, we return the accumulated \lstinline|result| map.

\subsubsection{Imperative DSL}
This piece of imperative code is implemented as a DSL, i.e.\ the imperative code is translated to a functional equivalent.

One may reasonably wonder why one would ever use a purely functional language to then write imperative code, translate it back to purely functional code, only to then compile the result to imperative machine code. But since Lean 4 is also an interactive theorem prover, the answer is simple: Imperative programming can be convenient, but it is easier to use a purely functional programming language for all the domains of proof, specification and writing programs at once, since all the computational effects are already neatly packed away. 

In fact, if we were to write a proof about \lstinline|Array.groupBy|, the first thing we would likely do is fold away the syntactical imperative layer in order to uncover the purely functional term representing the program, without ever having to think about loop invariants or state.

One may also wonder whether code written in this imperative manner is about as efficient as real imperative code. Because of the mechanism described in \cref{sec:beans}, this is indeed the case if the code in question is written in such a way that every value is uniquely referenced, which is usually the case for the kind of code that one would also write in an imperative language. Importantly, functional code will benefit from this as well, which means that we can write compositional and functional code that also updates values in-place instead of making new allocations in every single combinator. 

It must however be noted that our implementation of \lstinline|Array.groupBy| is actually an example of an imperative implementation that is unexpectedly inefficient. We will resolve this inefficiency in \cref{sec:beans} when it is instrumental to do so.

\subsubsection{Lean as a functional language}

For most of this thesis, we will not treat Lean 4 as a theorem prover, but instead as a purely functional programming language that implements dependent type theory. For more details on Lean 4 as a programming language, we refer to the book Functional Programming in Lean by \cite{christiansen_functional_2023}.

\section{Dependent Type Theory}\label{sec:dtt}
As described in \cref{sec:lean4}, Lean uses a single language for programming and proving. It accomplishes this by implementing dependent type theory (DTT), a type theory powerful enough to declare mathematical objects, implement programs and write specifications and proofs for both. What follows is only a very brief introduction to some of the important details of Lean's type theory. We recommend the book Theorem Proving in Lean 4 by \cite{avigad_theorem_2022} for a proper introduction.

\subsubsection{Core mechanisms}
The central idea of DTT is that types are allowed to depend on terms. In most type theories, terms and types are entirely different constructs, and while terms have types, terms cannot be used in types. Removing this restriction blurs the line between programs and their static specification.

There are several mechanisms required to make this work: 
\begin{enumerate}
	\item In a quantified type $\forall x.\ \tau(x)$, the variable $x$ is allowed to range over terms of a type (e.g.\ $x \triangleq n : \mathbb{N}$), not just types themselves as is the case in languages that support polymorphic functions $\forall \alpha.\ \tau(\alpha)$.
	\item Terms in types can be reduced with the usual reduction rules of lambda calculus, e.g.\ $(\forall x.\ \tau((\lambda y.\ y)\ x)) \rightsquigarrow (\forall x.\ \tau(x))$.
	\item Support for inductive type families, which are essentially algebraic data types where each constructor creates a term in a type that can be parametrized by other terms. For example, we might declare a type $\lambda \alpha : \mathrm{Type}.\ \lambda n : \mathbb{N}.\ \mathrm{Vec}\ \alpha\ n$ for lists over a type $\alpha$ of size $n$ with constructors $\mathrm{nil} : \mathrm{Vec}\ \alpha\ 0$ and $\mathrm{cons} : \forall n : \mathbb{N}.\ \alpha \to \mathrm{Vec}\ \alpha\ n \to \mathrm{Vec}\ \alpha\ (n+1)$. Each inductive type family also yields a recursion principle that allows for pattern matching and using recursion on the value of a type to compute an accumulate value.
\end{enumerate}

In addition to $\forall x : \tau_1.\ \tau_2(x)$, dependent type theories also always support dependent sigma types $(x : \tau_1) \times \tau_2(x)$ and sum types $\tau_1 + \tau_2$.

\subsubsection{Propositions}
For convenience, Lean's type theory also supports a separate type universe of propositions $\mathbb{P}$, the terms of which are types $p : \mathbb{P}$ with proof terms $h : p$ witnessing the truth of the proposition $p$. For example, if $\mathrm{refl} : \forall x.\ x = x$, we have $(\mathrm{refl}\ n) : (n = n) : \mathbb{P}$ and $(\mathrm{refl}\ n) : (n + 1 - 1 = n) : \mathbb{P}$ for $n : \mathbb{N}$, as $n + 1 - 1 \rightsquigarrow n$ by reduction. Meanwhile, there is no term $h : (n = n + 1) : \mathbb{P}$. For less trivial propositions, we use the recursion principles of inductive type families, which become induction principles if the accumulated value is a proposition $p : \mathbb{P}$. 

What distinguishes propositions $p : \mathbb{P}$ from other types is that $\mathbb{P}$ is impredicative and proof-irrelevant, i.e.\ whenever we quantify over a proposition $p : \mathbb{P}$ s.t.\ $\forall x.\ p$, the resulting type is again a proposition, and for proofs $h_1, h_2 : p$, we have $h_1 = h_2$. In other words, propositions are contained to $\mathbb{P}$ and all proofs of a proposition are considered equal, i.e.\ only their existence is relevant, not the concrete content of the proof. Hence, proofs are inherently non-computational; since the content of a proof is irrelevant, it can be erased. Lean then also introduces additional non-computational classical axioms into its universe of propositions $\mathbb{P}$, most prominently the law of the excluded middle $p \lor \lnot p$.

\subsubsection{Combined power}
Putting all of these mechanisms together yields a type theory powerful enough to declare types like $\mathrm{Vec}\ \alpha\ n$ and all the usual objects that are used in mathematics, as well as logical operators like $\cdot \land \cdot$, $\exists x.\ p(x)$, $\cdot = \cdot$ and even well-founded recursion. Additionally, the term language is strong enough to write arbitrary programs, as well as classical proofs within $\mathbb{P}$.

For a detailed formal description of Lean's type theory, we refer to \citep{carneiro_type_2019}.

\section{Intermediate Representations for Lean 4}\label{sec:irs}
As outlined in \cref{sec:lean4}, Lean 4 ultimately compiles to either C code or LLVM IR code \citep{lattner_llvm_2004}, with the latter being work-in-progress. Once compiled, much of the structure of the original Lean 4 program is lost, including types and the global guarantee of purity. Hence, introducing additional intermediate representations between Lean 4 and the resulting compiled program can help with leveraging the additional structure in the original Lean 4 code for a first pass of Lean-specific optimizations.

\subsubsection{Pure IR}
Since very early on in the development of Lean 4, the Lean compiler has compiled to the intermediate representations described by \cite{ullrich_counting_2020}, with optimizations like common subexpression elimination \citep{cocke_global_1970}, verified rewrite rules \citep{jones_playing_2001}, lambda lifting \citep{johnsson_lambda_1985}, erasure of computationally-irrelevant terms \citep{tejiscak_erasure_2019}, specialization \citep{augustsson_implementing_1993} and inlining \citep{jones_secrets_2002} being performed on Lean 4 expressions before compiling to the IR.

Instead, the IR is used to implement reference counting. Programs compile to the untyped IR defined below, commonly called ``the pure IR'': 
\newcommand{\sep}{\ \ |\ \ }
\newcommand{\icode}[1]{\textrm{\lstinline[language=ir-if]|#1|}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Ctor}{\mathrm{Ctor}}
\newcommand{\Proj}{\mathrm{Proj}}
\newcommand{\Const}{\mathrm{Const}}
\newcommand{\Expr}{\mathrm{Expr}}
\newcommand{\FnBody}{\mathrm{FnBody}}
\newcommand{\Fn}{\mathrm{Fn}}
\newcommand{\Program}{\mathrm{Program}}
\begin{alignat*}{3}
	\icode{x}, \icode{y} &\in \Var_\lambda \\
	\icode{i} &\in \Ctor_\lambda \\
	\icode{j} &\in \Proj_\lambda \\
	\icode{c} &\in \Const_\lambda \\
	\icode{e} &\in \Expr_\lambda &\Coloneqq&\ \icode{c\ $\overline{\texttt{y}}$}
	\sep \icode{pap c\ $\overline{\texttt{y}}$}
	\sep \icode{x y}
	\sep \icode{ctorᵢ\ $\overline{\texttt{y}}$}
	\sep \icode{projᵢⱼ y} \\
	\icode{F} &\in \FnBody_\lambda &\Coloneqq&\ \icode{ret x}
	\sep \icode{let x := e; F}
	\sep \icode{case x of\ $\overline{\texttt{F}}$} \\
	f &\in \Fn_\lambda &\Coloneqq&\ \lambda\ \overline{\texttt{y}}.\ \icode{F} \\
	\delta &\in \Program_\lambda &=&\ \Const_\lambda \rightharpoonup \Fn_\lambda
\end{alignat*}
$\overline{\texttt{y}}$ is a vector of variables \icode{y}. Expressions are full applications \icode{c\ $\overline{\texttt{y}}$}, partial applications \icode{pap c\ $\overline{\texttt{y}}$}, variable applications \icode{x y} of a variable \icode{y} to a higher-order function \icode{x} created by \icode{pap}, constructor applications \icode{ctorᵢ\ $\overline{\texttt{y}}$} for a constructor \icode{i} or projections \icode{projⱼ y} for a field \icode{j} in \icode{y}. Function bodies consist of \icode{ret x}, \icode{let x := e; F} and a case-instruction \icode{case x of\ $\overline{\texttt{F}}$} that makes \icode{x} denote the \icode{i}-th constructor in \icode{$\overline{\texttt{F}}$ᵢ}. All functions are lifted out to a global map $\delta \in \Program_\lambda$.

\subsubsection{Reference-counted IR}
The Lean 4 compiler then inserts the following additional impure instructions into code in pure IR form:
\begin{alignat*}{3}
	\icode{e} &\in \Expr &\Coloneqq&\ \dots \sep \icode{reset x} \sep \icode{reuse x in ctorᵢ\ $\overline{\texttt{y}}$} \\
	\icode{F} &\in \FnBody\ &\Coloneqq&\ \dots \sep \icode{inc x; F} \sep \icode{dec x; F}
\end{alignat*}
The resulting IR is also commonly called ``the reference-counted IR''. We will cover \icode{reset x} and \icode{reuse x in ctorᵢ\ $\overline{\texttt{y}}$} in detail in \cref{sec:beans}. \icode{inc x; F} and \icode{dec x; F} increment and decrement the reference count of the value \texttt{x} respectively. When the reference count reaches 0, the value in question is not referenced anymore and can be freed safely. Since Lean 4 is a strict purely functional programming language, there can be no reference cycles.

When compiling the pure IR to the reference-counted IR, the Lean compiler performs additional optimizations: Parameters are sometimes inferred as ``borrowed'', which means that the caller is keeping these parameters alive, and so they need not be reference-counted in the callee. We will encounter a similar but slightly different notion of ``borrowing'' in \cref{sec:borrowingbackground}, albeit both have in common that the caller is managing a resource for a callee.

In addition to the instructions described by \cite{ullrich_counting_2020}, Lean's implementation of this IR also supports operations for boxing and unboxing of scalar values \citep{henglein_formally_1994}, as well as join point declarations and corresponding jump instructions that allow the re-joining of control flow branches \citep{maurer_compiling_2017}. In other words, Lean's IR is implemented in A-normal form as described by \cite{maurer_compiling_2017}, where functions can only take variables as arguments, nested expressions are factored out into let-expressions and join points are generated where possible.

We will henceforth ignore the former as an implementation detail and briefly discuss the latter in \cref{sec:ir} and \cref{sec:inference}.

\subsubsection{LCNF}
While the IRs defined above are well suited for implementing garbage collection and optimizations associated with reference counting, they lack types and hence much of the structure of the original Lean 4 program, which is why the current Lean 4 compiler performs other optimizations directly on Lean expressions. 

In the current rewrite of the Lean 4 compiler toolchain that is still work-in-progress, another IR, the Lean Compiler Normal Form (LCNF), is added to mitigate this issue. It is also stated in A-normal form and defined by the following grammar:
\newcommand{\LeanType}{\mathrm{LeanType}}
\newcommand{\LCNF}{\mathrm{LCNF}}
\newcommand{\DeclName}{\mathrm{DeclName}}
\newcommand{\Arg}{\mathrm{Arg}}
\newcommand{\LetValue}{\mathrm{LetValue}}
\newcommand{\Case}{\mathrm{Case}}
\newcommand{\Code}{\mathrm{Code}}
\newcommand{\Decl}{\mathrm{Decl}}
\begin{alignat*}{3}
	\tau &\in \LeanType \\
	\icode{D} &\in \DeclName_\LCNF \\
	\icode{x}, \icode{y} &\in \Var_\LCNF \\
	\icode{i} &\in \Ctor_\LCNF \\
	\icode{j} &\in \Proj_\LCNF \\
	\icode{a} &\in \Arg_\LCNF &\Coloneqq&\ \blacksquare \sep \icode{y} \sep \tau \\
	\icode{v} &\in \LetValue_\LCNF\ &\Coloneqq&\ \blacksquare
		\sep \icode{D\ $\overline{\texttt{a}}$}
		\sep \icode{y\ $\overline{\texttt{a}}$}
		\sep \icode{proj$_\texttt{j}$ y} \\
	\icode{c} &\in \Case_\LCNF &\Coloneqq&\ \icode{ctorᵢ\ $\overline{\texttt{(y}\ \texttt{:}\ \tau_\texttt{y}\texttt{)}}$ ⇒ K} 
		\sep \icode{default ⇒ K} \\
	\icode{K} &\in \Code_\LCNF &\Coloneqq&\ \icode{let (x :\ $\tau$) := v; K} \\
		&&&\enspace\sep \icode{((def x\ $\overline{\texttt{(y}\ \texttt{:}\ \tau_\texttt{y}\texttt{)}}$ := K$_\texttt{x}$) :\ $\tau$); K} \\
		&&&\enspace\sep \icode{((jpdef x\ $\overline{\texttt{(y}\ \texttt{:}\ \tau_\texttt{y}\texttt{)}}$ := K$_\texttt{x}$) :\ $\tau$); K} \\
		&&&\enspace\sep \icode{jmp x\ $\overline{\texttt{a}}$} \\
		&&&\enspace\sep \icode{((case x of\ $\overline{\texttt{c}}$) :\ $\tau$)} \\
		&&&\enspace\sep \icode{ret x} \\
	\icode{d} &\in \Decl_\LCNF &\Coloneqq&\ (\lambda \overline{\texttt{(y}\ \texttt{:}\ \tau_\texttt{y}\texttt{)}}.\ \texttt{K}\ :\ \tau) \\
	\delta &\in \Program_\LCNF &=&\ \DeclName_\LCNF \rightharpoonup \Decl_\LCNF
\end{alignat*}
We have omitted some technical details from Lean's implementation of this grammar that are not relevant to this thesis. 

LeanType denotes arbitrary dependent types in the form of generic Lean expressions. Declarations, variables, constructors and projections are all identified by names in their respective syntactical category. Arguments to declarations can either be erased ($\blacksquare$), variables or types. Let values can either be erased, an application of a constant declaration, an application of arguments to a variable that may contain a declaration or a function, or a projection of a variable that stores a type with only a single constructor. Cases are either destructuring patterns that match against a constructor \texttt{i} and denote its fields using variables $\overline{\texttt{y}}$ with types $\overline{\tau}$, or a \icode{default} case.

Code instructions can either be \icode{let} instructions, function definitions \icode{def} of type $\tau$, join point declarations \icode{jpdef} of type $\tau$, \icode{jmp} instructions that jump to a given join point with a given vector of arguments, \icode{case} or \icode{ret} instructions.

After compiling Lean code to LCNF, the resulting LCNF code is simplified by many of the same optimizations that are applied to Lean code in the old compiler that works directly on Lean expressions. Near the end of the optimization phase, type dependencies, computationally irrelevant terms and a few other types are heuristically erased and the resulting LCNF code can be converted to the pure IR. In the future, both the pure and the reference-counted IR will be replaced by a closely related LCNF-equivalent as well. 

\section{Destructive Updates}\label{sec:beans}
In \cref{sec:irs}, we saw that the Lean 4 compiler inserts instructions \icode{let x := reset y} and \icode{let z := reuse x in ctorᵢ\ $\overline{\texttt{y}}$} when compiling the pure IR to the reference-counted IR. These instructions are used to perform so-called destructive updates: In a purely functional language, if a value is only referenced by a single function, then that function can safely update the value in-place without the mutation being observed by other functions, thus retaining both purity and efficiency. It is obvious that if we are using reference counting for garbage collection, then we can use the reference count for performing destructive updates by checking that the reference count of the value is equal to 1.

\subsubsection{Reset and reuse}
But one important question remains: In a purely functional language, what constitutes an update? After all, there is no dedicated instruction that performs updates. To answer this question, \cite{ullrich_counting_2020} introduce what they call ``the resurrection hypothesis'': Many values are used for the last time just before a value of the same kind is created. This should be intuitive to most functional programmers, as functions that would otherwise mutate a value in an imperative language will instead create a new instance of the old value with the change applied to it in purely functional languages.

As per \cite{ullrich_counting_2020}, the \icode{let x := reset y} instruction is inserted as early as possible when \texttt{y} is not used any more for the remainder of the function and a corresponding \icode{let z := ctorᵢ\ $\overline{\texttt{y}}$} call of the same kind as \texttt{y} exists later on in the same function. \icode{let z := reuse x in ctorᵢ\ $\overline{\texttt{y}}$} is used to replace the latter \icode{let z := ctorᵢ\ $\overline{\texttt{y}}$} call. Here, ``of the same kind'' means that both constructors occupy the same amount of memory.

Semantically, \icode{let x := reset y} checks the reference count of \texttt{y} and denotes in \texttt{x} both \texttt{y} itself and whether \texttt{y} had a reference count equal to 1. Additionally, if the reference count is equal to 1, \icode{reset y} detaches the components of the soon-to-be-freed \texttt{y} pre-emptively by decrementing the reference counts of the components of \texttt{y}. \icode{reuse x in ctorᵢ\ $\overline{\texttt{y}}$} then checks \texttt{x} and either updates the referenced memory in-place if the reference count was equal to 1 at the call to \icode{reset} or allocates new memory according to \icode{ctorᵢ\ $\overline{\texttt{y}}$}.

The fact that there are two separate instructions for checking the reference count, detaching the components and performing the in-place update is crucial, as no destructive updates can be performed on the components of a value as long as the value itself is still alive. This is evidenced by the following example taken from \cite{ullrich_counting_2020}:\\
\begin{ifcode}
map f xs = case xs of
  (ret xs)
  (let x = proj₁ xs;
   inc x;
   let s = proj₂ xs;
   inc s;
   let w = reset xs;
   let y = f x;
   let ys = map f s;
   let r = reuse w in ctor₂ y ys;
   ret r)
\end{ifcode}
If \icode{reuse} was to check the reference count of \texttt{xs}, then \texttt{s} would have a reference count of 2: One reference from \texttt{s} itself, and one from \texttt{xs}, thus preventing further destructive updates in the recursive call to \texttt{map}.

\subsubsection{Updating values within values}
The optimization above usually works if the functional code in question is written in a manner that is similar to clean imperative code, where old values are not used after updating them and references are not duplicated in such a manner that a mutation is observable in an entirely different part of the program. In fact, since this optimization also applies to code written with functional combinators like \lstinline|map| or \lstinline|filter|, even code that would otherwise be inefficient in imperative languages because explicit copies are created to maintain local purity can be optimized to be as fast as if it was written in an imperative manner. There are however some common exceptions to this rule, one of which we have already seen in detail in \cref{sec:lean4}:\\
\begin{code}
def Array.groupBy (p : α → α → Ordering) (xs : Array α)
  : RBMap α (Array α) p := Id.run do
  let mut result : RBMap α (Array α) p := RBMap.empty
  for x in xs do
    let group := Option.getD (RBMap.find? p result x) #[]
    result := RBMap.insert p result x (Array.push group x)
  return result
\end{code}

Here, after executing the line containing \lstinline|RBMap.find? p result x|, there can be two references to the array in \lstinline|group|: One from \lstinline|group| itself, and one within the red-black map \lstinline|result|. This precludes Lean from performing a destructive update in \lstinline|Array.push group x|, resulting in an expensive $\Theta(n)$-complexity copy of the whole array. Since this issue occurs every single loop iteration, our \lstinline|Array.groupBy| implementation is accidentally quadratic. Our imperative intuition lead us astray: if \lstinline|Array.push group x| was to update \lstinline|group| in-place, then this mutation would be observable in \lstinline|result|, but since we never observe it and immediately replace the reference to \lstinline|group| in \lstinline|result| using \lstinline|RBMap.insert p result x ...|, we consider the mutation safe. 

The common solution to this issue is to introduce a function similar to the following for any type that can act as a container:\\
\begin{code}
def RBMap.update (p : α → α → Ordering) (map : RBMap α β p) 
  (a : α) (f : β → β) : RBMap α β p :=
  let x := RBMap.find? p map a
  match x with
  | Option.none   => map
  | Option.some b => 
    let map := RBMap.erase p map a
    RBMap.insert p map a (f b)
\end{code}

\lstinline|match x with| uses pattern-matching on \lstinline|x|. After calling \lstinline|RBMap.find? p map a|, there are two references to the value in \lstinline|b|. Using \lstinline|RBMap.erase p map a|, the second reference is erased and \lstinline|b| can now be updated destructively in \lstinline|f|. Afterwards, the result is re-inserted. Using this function, we can now fix the performance bug in \lstinline|Array.groupBy| with the following implementation:\\
\begin{code}
def Array.groupBy (p : α → α → Ordering) (xs : Array α)
  : RBMap α (Array α) p := Id.run do
  let mut result : RBMap α (Array α) p := RBMap.empty
  for x in xs do
    if ! RBMap.contains p result x then
      result := RBMap.insert p result x #[]
    else
      result := RBMap.update p result x 
        (fun group => Array.push group x)
  return result
\end{code}

\lstinline|(fun x => ...)| denotes a lambda expression. As we are using \lstinline|RBMap.update|, the \lstinline|group| corresponding to \lstinline|x| is updated in-place, and now our implementation has the time complexity that we would expect it to have.

Situations akin to this one can occur for other types as well. We will discuss similar challenges related to containers that occur when ensuring referential uniqueness statically in \cref{sec:borrowingbackground}.

\section{Linear Type Theory}\label{sec:ltt}
\subsubsection{Substructural rules}
Type systems typically guarantee certain functional or extensional properties for a given program using a typing relation $\Gamma \vdash e : \tau$, where $\Gamma$ is a set of type judgements $x : \tau'$. The following \textsc{Exchange}, \textsc{Weaken} and \textsc{Contract} rules are usually assumed implicitly:
\begin{mathpar}
	$\inferrule[Exchange]{\Gamma_1, y : \tau_2, x : \tau_1, \Gamma_2 \vdash e : \tau}{\Gamma_1, x : \tau_1, y : \tau_2, \Gamma_2 \vdash e : \tau}$ \hspace{1.5em}
	$\inferrule[Weaken]{\Gamma \vdash e : \tau}{\Gamma, x : \tau' \vdash e : \tau}$ \hspace{1.5em}
	$\inferrule[Contract]{\Gamma, x : \tau', x : \tau' \vdash e : \tau}{\Gamma, x : \tau' \vdash e : \tau}$
\end{mathpar} 
In other words, judgements in the context can be reordered, discarded and duplicated freely.

In the formal descriptions of dependent type theories, like the one described in \cref{sec:dtt}, \textsc{Exchange} is inhibited, because $\tau_2$ may depend on $x$, which induces an order of declaration on variables in the context. But if we want to guarantee only extensional properties for $e$, \textsc{Weaken} and \textsc{Contract} can always be freely assumed, as there is nothing to gain from retaining the exact count of every judgement in the context. 

However, if we wish to guarantee non-functional or intensional properties for a given program, then discarding the \textsc{Weaken} and \textsc{Contract} rules can be useful. And indeed doing just that constitutes the core idea of substructural type theories: By retaining the exact amount of each judgement in the context, we can use typing rules to count objects in our program to ensure various kinds of intensional properties.

\subsubsection{Beginnings of linear type theory}
In most substructural type theories, the extra detail in the context is used to count variable uses. \cite{girard_linear_1987} was the first to notice that not assuming \textsc{Weaken} and \textsc{Contract} allows one to define so-called linear logics with dualities that allow reasoning about resource usage, and \cite{wadler_linear_1990} transports this idea to form a linear type theory with a set of entirely separate linear and non-linear types, even on the term level. See \cref{fig:wadler-original} for Wadler's original linear type system.

\subsubsection{Properly linear or invariably unique}
\cite{wadler_is_1991} then goes on to define the \textsc{Dereliction} and \textsc{Promotion} rules listed below that allow coercing a non-linear type to a linear type and promoting a linear type to a nonlinear one if it only depends on nonlinear types, as well as a ``steadfast'' type system where there is no coercion between linear and non-linear types, but both use the same term language. Especially in the former, non-linear types can be understood as having an unlimited quantity of something, whereas linear types can be understood as having exactly one of something.
\begin{mathpar}
	$\inferrule[Dereliction]{\Gamma, x : \tau_1 \vdash e : \tau_2}{\Gamma, x :\ !\tau_1 \vdash e : \tau_2}$ \hspace{1.5em}
	$\inferrule[Promotion]{!\Gamma \vdash e : \tau}{!\Gamma \vdash e :\ !\tau}$
\end{mathpar}
Here, $!\tau$ is a nonlinear type and $!\Gamma$ refers to every type in the context $\Gamma$ being non-linear. The motivation for including \textsc{Dereliction} is that $!$ can be understood as having an unlimited quantity of something, and if you have an unlimited quantity of something, you also have one of something. \textsc{Promotion} is justified because any single resource that can be created from resources of unlimited quantity can also be created in unlimited quantity by repeating the process.

With \textsc{Dereliction} and \textsc{Promotion}, linear types make a guarantee for the future: As it could have been coerced from non-linear type, we do not know whether this variable has always been linear, but now that it is, we guarantee that it will be used exactly once. Meanwhile, in Wadler's ``steadfast'' version of the type system, as there is no coercion from non-linear to linear types, a linear variable has and will always be used exactly once.

In a type system with \textsc{Dereliction}, linearity is unsuited to guarantee the referential uniqueness of a variable, as it may have been duplicated in the past, an issue that is acknowledged by \cite{wadler_is_1991}. For linear type systems where linearity is guaranteed from construction onwards, \cite{chirimar_reference_1996} prove that the linearity of a variable implies the uniqueness of the associated reference.

Future linear type systems \citep{goos_observers_1992} \citep{atkey_syntax_2018} \citep{bernardy_linear_2018} \citep{brady_idris_2021} \citep{choudhury_graded_2021} \citep{li_linear_2022} \citep{spiwack_linearly_2022} always adopt one of these two approaches and either allow for a non-linear to linear coercion, or no coercion at all, thus cementing the idea of ``linearity'' referring to either ``always uniquely referenced'' or ``used linearly from this point onwards''. The latter can be made to act like the former by guaranteeing referential uniqueness through other means, e.g.\ by making every constructor of a certain type return a value of linear type, effectively turning referential uniqueness into a library design decision.

To make the distinction between the two kinds of linear type systems clear, we will henceforth refer to linear type systems without any coercion between linear and non-linear values as ``invariably unique'' and type systems adopting \textsc{Dereliction} and \textsc{Promotion} as ``properly linear'', but still use the term ``linear'' to group the two of them together, as is often done in the literature since Wadler's first papers. See \cref{fig:wadler-properly-linear} for Wadler's properly linear type system and \cref{fig:wadler-invariably-unique} for his invariably unique type system.

\subsubsection{Affinity}
One common refinement of linear type theory is to allow the use of \textsc{Weaken}, but not \textsc{Contract}, specifically when the type theory only wants to guarantee that a reference is unique, but not that it is used. These type theories are also knows as ``affine'' \citep{tov_practical_2011}, though the term is often conflated with ``linear''.

\subsubsection{Applications}
The various applications of linear type theory include the following:
\begin{itemize}
	\item Threading a functional program and enforcing an execution order to replace the use of monads for I/O in functional languages with a linear equivalent \citep{de_vries_making_2009} \citep{bernardy_linear_2018} \citep{brady_idris_2021}
	\item Ensuring resource- and memory-safety so that resources and memory cannot be freed multiple times \citep{weiss_oxide_2021}
	\item Performing efficient in-place updates and enabling the use of arrays in functional languages \citep{de_vries_making_2009} \citep{bernardy_linear_2018}
	\item Specifying usage protocols for types \citep{brady_idris_2021}
	\item Guiding program synthesis \citep{brady_idris_2021}
	\item Inverting the computation of functions \citep{matsuda_sparcl_2020}
\end{itemize}
\clearpage
\newcommand{\acode}[1]{\textrm{\lstinline[language=aux]|#1|}}
\topskip0pt
\vspace*{\fill}
\begin{mdframed}
\begin{figure}[H]
	\vspace{-2em}
	\begin{mathpar}
		$\inferrule[Var]{ }{x : \tau \vdash x : \tau}$ \hspace{1.5em}
		$\inferrule[$\multimap$-Intro]{\Gamma, x : \tau_1 \vdash e : \tau_2}{\Gamma \vdash (1\ \lambda x : \tau_1.\ e) : \tau_1 \multimap \tau_2}$ \hspace{1.5em}
		$\inferrule[$\multimap$-Elim]{\Gamma_1 \vdash e_1 : \tau_1 \multimap \tau_2 \\ \Gamma_2 \vdash e_2 : \tau_1}{\Gamma_1, \Gamma_2 \vdash (1\ e_1\ e_2) : \tau_2}$
	\end{mathpar}
	\begin{mathpar}
		$\inferrule[Weaken]{\Gamma \vdash e : \tau \\ \tau'\ \mathrm{nonlinear}}{\Gamma, x : \tau' \vdash e : \tau}$ \hspace{1.5em}
		$\inferrule[Contract]{\Gamma, x : \tau', x : \tau' \vdash e : \tau \\ \tau'\ \mathrm{nonlinear}}{\Gamma, x : \tau' \vdash e : \tau}$
	\end{mathpar}
	\begin{mathpar}
		$\inferrule[$\to$-Intro]{\Gamma, x : \tau_1 \vdash e : \tau_2 \\ \Gamma\ \mathrm{nonlinear}}{\Gamma \vdash (!\ \lambda x : \tau_1.\ e) : \tau_1 \to \tau_2}$ \hspace{1.5em}
		$\inferrule[$\to$-Elim]{\Gamma_1 \vdash e_1 : \tau_1 \to \tau_2 \\ \Gamma_2 \vdash e_2 : \tau_1}{\Gamma_1, \Gamma_2 \vdash (!\ e_1\ e_2) : \tau_2}$
	\end{mathpar}
	\caption{Original linear type system from \cite{wadler_linear_1990} with separate linear and non-linear terms and types. $1$ denotes linear terms, $!$ denotes non-linear terms. ``nonlinear'' demands that the full type or context consists only of non-linear $\to$-types.}
	\label{fig:wadler-original}
\end{figure}
\end{mdframed}
\vspace*{\fill}
\begin{mdframed}
\begin{figure}[H]
	\vspace{-2em}
	\begin{mathpar}
		$\inferrule[Weaken]{\Gamma \vdash e : \tau}{\Gamma, x :\ !\tau' \vdash e : \tau}$ \hspace{1.5em}
		$\inferrule[Contract]{\Gamma, x :\ !\tau', x :\ !\tau' \vdash e : \tau}{\Gamma, x :\ !\tau' \vdash e : \tau}$ \hspace{1.5em}
		$\inferrule[Dereliction]{\Gamma, x : \tau_1 \vdash e : \tau_2}{\Gamma, x :\ !\tau_1 \vdash e : \tau_2}$ \hspace{1.5em}
		$\inferrule[Promotion]{!\Gamma \vdash e : \tau}{!\Gamma \vdash e :\ !\tau}$ \hspace{1.5em}
	\end{mathpar} 
	\begin{mathpar}
		$\inferrule[Var]{ }{x : \tau \vdash x : \tau}$ \hspace{1.5em}
		$\inferrule[$\multimap$-Intro]{\Gamma, x : \tau_1 \vdash e : \tau_2}{\Gamma \vdash \lambda x : \tau_1.\ e : \tau_1 \multimap \tau_2}$ \hspace{1.5em}
		$\inferrule[$\multimap$-Elim]{\Gamma_1 \vdash e_1 : \tau_1 \multimap \tau_2 \\ \Gamma_2 \vdash e_2 : \tau_1}{\Gamma_1, \Gamma_2 \vdash e_1\ e_2 : \tau_2}$
	\end{mathpar}
	\begin{mathpar}
		$\inferrule[$\otimes$-Intro]{\Gamma_1 \vdash e_1 : \tau_1 \\ \Gamma_2 \vdash e_2 : \tau_2}{\Gamma_1, \Gamma_2 \vdash (e_1, e_2) : \tau_1 \otimes \tau_2}$ \hspace{1.5em}
		$\inferrule[$\otimes$-Elim]{\Gamma_1 \vdash e_1 : \tau_1 \otimes \tau_2 \\ \Gamma_2, x : \tau_1, y : \tau_2 \vdash e_2 : \tau_3}{\Gamma_1, \Gamma_2 \vdash \acode{case}\ e_1\ \acode{of}\ (x, y) \Rightarrow e_2 : \tau_3}$
	\end{mathpar}
	\begin{mathpar}
		$\inferrule[$\oplus$-Intro-Left]{\Gamma \vdash e : \tau_1}{\Gamma \vdash \acode{left}\ e : \tau_1 \oplus \tau_2}$ \hspace{1.5em}
		$\inferrule[$\oplus$-Intro-Right]{\Gamma \vdash e : \tau_2}{\Gamma \vdash \acode{right}\ e : \tau_1 \oplus \tau_2}$ 
	\end{mathpar}
	\begin{mathpar}
		$\inferrule[$\oplus$-Elim]{\Gamma_1 \vdash e_1 : \tau_1 \oplus \tau_2 \\ \Gamma_2, x : \tau_1 \vdash e_2 : \tau_3 \\ \Gamma_2, y : \tau_2 \vdash e_3 : \tau_3}{\Gamma_1, \Gamma_2 \vdash \acode{case}\ e_1\ \acode{of}\ \acode{left}\ x \Rightarrow e_2;\ \acode{right}\ y \Rightarrow e_3 : \tau_3}$
	\end{mathpar}
	\caption{Properly linear type system from \cite{wadler_is_1991} with support for multiplicative product types ($\otimes$) and multiplicative sum types ($\oplus$).}
	\label{fig:wadler-properly-linear}
\end{figure}
\end{mdframed}
\vspace*{\fill}
\clearpage
\topskip0pt
\vspace*{\fill}
\begin{mdframed}
\begin{figure}[H]
	\vspace{-2em}
	\begin{mathpar}
		$\inferrule[Weaken]{\Gamma \vdash e : \tau}{\Gamma, x :\ !\tau' \vdash e : \tau}$ \hspace{1.5em}
		$\inferrule[Contract]{\Gamma, x :\ !\tau', x :\ !\tau' \vdash e : \tau}{\Gamma, x :\ !\tau' \vdash e : \tau}$ 
	\end{mathpar} 
	\begin{mathpar}
		$\inferrule[Var]{ }{x : \tau \vdash x : \tau}$ \hspace{1.5em}
		$\inferrule[$\multimap$-Intro]{\Gamma, x : \tau_1 \vdash e : \tau_2}{\Gamma \vdash \lambda x : \tau_1.\ e : \tau_1 \multimap \tau_2}$ \hspace{1.5em}
		$\inferrule[$\multimap$-Elim]{\Gamma_1 \vdash e_1 : \tau_1 \multimap \tau_2 \\ \Gamma_2 \vdash e_2 : \tau_1}{\Gamma_1, \Gamma_2 \vdash e_1\ e_2 : \tau_2}$
	\end{mathpar}
	\begin{mathpar}
		$\inferrule[$\otimes$-Intro]{\Gamma_1 \vdash e_1 : \tau_1 \\ \Gamma_2 \vdash e_2 : \tau_2}{\Gamma_1, \Gamma_2 \vdash (e_1, e_2) : \tau_1 \otimes \tau_2}$ \hspace{1.5em}
		$\inferrule[$\otimes$-Elim]{\Gamma_1 \vdash e_1 : \tau_1 \otimes \tau_2 \\ \Gamma_2, x : \tau_1, y : \tau_2 \vdash e_2 : \tau_3}{\Gamma_1, \Gamma_2 \vdash \acode{case}\ e_1\ \acode{of}\ (x, y) \Rightarrow e_2 : \tau_3}$
	\end{mathpar}
	\begin{mathpar}
		$\inferrule[$\oplus$-Intro-Left]{\Gamma \vdash e : \tau_1}{\Gamma \vdash \acode{left}\ e : \tau_1 \oplus \tau_2}$ \hspace{1.5em}
		$\inferrule[$\oplus$-Intro-Right]{\Gamma \vdash e : \tau_2}{\Gamma \vdash \acode{right}\ e : \tau_1 \oplus \tau_2}$ 
	\end{mathpar}
	\begin{mathpar}
		$\inferrule[$\oplus$-Elim]{\Gamma_1 \vdash e_1 : \tau_1 \oplus \tau_2 \\ \Gamma_2, x : \tau_1 \vdash e_2 : \tau_3 \\ \Gamma_2, y : \tau_2 \vdash e_3 : \tau_3}{\Gamma_1, \Gamma_2 \vdash \acode{case}\ e_1\ \acode{of}\ \acode{left}\ x \Rightarrow e_2;\ \acode{right}\ y \Rightarrow e_3 : \tau_3}$
	\end{mathpar}
	\begin{mathpar}
		$\inferrule[!-$\multimap$-Intro]{!\Gamma, x : \tau_1 \vdash e : \tau_2}{!\Gamma \vdash \lambda x : \tau_1.\ e :\ !(\tau_1 \multimap \tau_2)}$ \hspace{1.5em}
		$\inferrule[!-$\multimap$-Elim]{\Gamma_1 \vdash e_1 :\ !(\tau_1 \multimap \tau_2) \\ \Gamma_2 \vdash e_2 : \tau_1}{\Gamma_1, \Gamma_2 \vdash e_1\ e_2 : \tau_2}$
	\end{mathpar}
	\begin{mathpar}
		$\inferrule[!-$\otimes$-Intro]{\Gamma_1 \vdash e_1 :\ !\tau_1 \\ \Gamma_2 \vdash e_2 :\ !\tau_2}{\Gamma_1, \Gamma_2 \vdash (e_1, e_2) :\ !(!\tau_1 \otimes\ !\tau_2)}$ \hspace{1.5em}
		$\inferrule[!-$\otimes$-Elim]{\Gamma_1 \vdash e_1 :\ !(!\tau_1 \otimes\ !\tau_2) \\ \Gamma_2, x :\ !\tau_1, y :\ !\tau_2 \vdash e_2 : \tau_3}{\Gamma_1, \Gamma_2 \vdash \acode{case}\ e_1\ \acode{of}\ (x, y) \Rightarrow e_2 : \tau_3}$
	\end{mathpar}
	\begin{mathpar}
		$\inferrule[!-$\oplus$-Intro-Left]{\Gamma \vdash e :\ !\tau_1}{\Gamma \vdash \acode{left}\ e :\ !(!\tau_1 \oplus\ !\tau_2)}$ \hspace{1.5em}
		$\inferrule[!-$\oplus$-Intro-Right]{\Gamma \vdash e :\ !\tau_2}{\Gamma \vdash \acode{right}\ e :\ !(!\tau_1 \oplus\ !\tau_2)}$ 
	\end{mathpar}
	\begin{mathpar}
		$\inferrule[!-$\oplus$-Elim]{\Gamma_1 \vdash e_1 :\ !(!\tau_1 \oplus\ !\tau_2) \\ \Gamma_2, x :\ !\tau_1 \vdash e_2 : \tau_3 \\ \Gamma_2, y :\ !\tau_2 \vdash e_3 : \tau_3}{\Gamma_1, \Gamma_2 \vdash \acode{case}\ e_1\ \acode{of}\ \acode{left}\ x \Rightarrow e_2;\ \acode{right}\ y \Rightarrow e_3 : \tau_3}$
	\end{mathpar}
	\caption{Invariably unique type system from \cite{wadler_is_1991}.}
	\label{fig:wadler-invariably-unique}
\end{figure}
\end{mdframed}
\vspace*{\fill}
\clearpage

\section{Quantitative Type Theory}\label{sec:qtt}
Quantitative type theory (QTT) applies the idea of properly linear type theory to dependent type theory. 

\subsubsection{Context-distribution problem}
In linear type theories, the \textsc{App} rule is typically stated in a manner similar to the following in order to distribute the needed amount of judgements to both expressions:
\begin{mathpar}
	$\inferrule[App]{\Gamma_1 \vdash e_1 : \tau_1 \multimap \tau_2 \\ \Gamma_2 \vdash e_2 : \tau_1}{\Gamma_1, \Gamma_2 \vdash e_1\ e_2 : \tau_2}$
\end{mathpar}

When omitting the \textsc{Exchange} rule in dependent type theory, it is not clear that splitting the context into $\Gamma_1$ and $\Gamma_2$ is always possible, since types in $\Gamma_2$ may depend on variables in $\Gamma_1$. This seemingly technical issue induces a semantic problem as well: Which occurrences of a variable constitute a use? For example, what about occurrences in types?

\subsubsection{Erasure}
After this issue was left unsolved for a long time, \cite{lindley_i_2016} resolved it by introducing a third kind of type to linear type theory, resulting in the three kinds of type ``linear'' (denoted as $1$), ``non-linear'' (denoted as $\omega$) and ``erased'' (denoted as $0$). ``erased'' specifies that a type or a term within DTT is not computationally relevant and will be erased by the compiler. 

Erased terms or types can only be used in other erased terms or types, and types are always erased. Since an erased term is not computationally relevant, we also do not need to count variable uses in erased terms. Finally, when a linear variable is used, it becomes erased, which justifies the use of $1$ to denote linear types and $0$ to denote erased types. 

\textsc{Exchange} is still omitted, but variables in the context that act as dependencies for other variables in the context are always guaranteed to be erased. In \textsc{App}, since we do not need to count the uses of variables of type $0$, we can freely distribute variables of type $1$ and $\omega$ to either $\Gamma_1$ or $\Gamma_2$, but duplicate all variables of type $0$ in their given order to $\Gamma_1$ and $\Gamma_2$. In other words, both the technical and the semantic issue are resolved at once.

\subsubsection{Quantities instead of types}
The implementations of QTT by \cite{lindley_i_2016} and \cite{atkey_syntax_2018} use an additional trick to make the description of the type theory more compact. 

While linear type theories usually denote linearity on the type $\tau$, quantitative type theory instead denotes it on the binder ``$:$'' in $x : \tau$, which leads to $0$, $1$ and $\omega$ not simply being kinds of types, but ``quantities'' on the binders $x :_q \tau$ for $q \in \{0, 1, \omega\}$ in the context. There are only linear types, and the quantity on the binder determines the substructural restrictions on the variable. 

The typing judgement $\Gamma \vdash e :_\sigma \tau$ is quantified as well to track whether $e$ is being checked in a computationally relevant or irrelevant context, and $\sigma$ is restricted to $\{0, 1\}$ to ensure admissibility of substitution \citep{atkey_syntax_2018}. 

With these tools, $\tau_1 \multimap \tau_2$ is encoded as $(x :_1 \tau_1) \to \tau_2$ and $\tau_1 \otimes \tau_2$ from \cref{fig:wadler-properly-linear} in \cref{sec:ltt} is encoded as $(x :_1 \tau_1) \otimes (y :_1 \tau_2) \otimes \mathbb{1}$. With a dependent type-level if-then-else construct, we can also encode $\tau_1 \oplus \tau_2 := (b :_1 \mathbb{B}) \otimes (\mathrm{if}\ b\ \mathrm{then}\ \tau_1\ \mathrm{else}\ \tau_2)$ \citep{grenrus_dependent_2020}.

Both \cite{lindley_i_2016} and \cite{atkey_syntax_2018} specify a generic framework for adding additional quantities to the type theory, allowing, for example, the additional introduction of a $\leq\hspace{-0.3em}1$ quantity representing affinity, or more accurate accounting of uses $n > 1$.

The typing rules for quantitative type theory as described by \cite{svoboda_additive_2021} and inspired by \cite{atkey_syntax_2018} can be found in \cref{fig:qtt}.

\subsubsection{Demand-based consumption}
Finally, note that by denoting the linear quantity on the binder in $(x :_1 \tau_1) \to \tau_2$, we cannot specify a quantity for $\tau_2$ any more, as we could in linear type theory by using either a linear or non-linear type for the return type. Instead, when checking an application $f\ (g\ e) :_1 \tau_3$ for $f :_1 (x :_q \tau_2) \to \tau_3$, $g :_1 (x :_1 \tau_1) \to \tau_2$ and $e :_1 \tau_1$, we demand $q$ instances of the resources $\Gamma$ required to check $g\ e :_1 \tau_2$. 

In other words, if we need $q$ instances of a return value that requires $\Gamma$ resources to produce, we instead demand $q \cdot \Gamma$ resources in our context, pretending that we applied the function $q$ times to obtain $q$ instances of the return value. This way, quantities need not be specified on return values, and the resources for the arguments are consumed based on the required amount of the return value. The \textsc{Promotion} rule from \cref{sec:ltt} is integrated into the rules for other types.

This trick is not inherent to quantitative types and can be applied to linear type systems of any kind by taking $f : \tau_1 \multimap \tau_2$ to mean ``$f$ consumes one of $\tau_1$ for every instance of $\tau_2$ that is required'', e.g.\ as in Linear Haskell \citep{bernardy_linear_2018} or in \cite{ghica_bounded_2014}. 

However, it is not entirely for free: If one wishes to both use a linear or quantitative type system with \textsc{Dereliction}, as well as guarantee referential uniqueness, then the lack of an annotation on the return type of functions means that we cannot simply annotate every constructor to return a linear type in order to ensure referential uniqueness, as described in \cref{sec:ltt}. 
Instead, both quantitative and linear type theories that use this trick and want to guarantee referential uniqueness define constructors in a continuation passing manner; e.g.\ $\mathrm{mkArray} : \mathbb{N} \multimap \alpha \multimap \mathrm{Array}\ \alpha$ becomes $\mathrm{mkArray} : \mathbb{N} \multimap \alpha \multimap (\mathrm{Array}\ \alpha \multimap\ !\tau) \multimap\ !\tau$ \citep{bernardy_linear_2018}.

\subsubsection{Applications}
Quantitative type theory combines the benefits of linear type theory and dependent type theory, leading to far greater capabilities when specifying protocols for types \citep{brady_idris_2021}. The introduction of an erasure quantity to combine the two type theories also allows for finer-grained specification of terms that are not computationally relevant but would otherwise be expensive to compute \citep{brady_idris_2021}.
\clearpage
\topskip0pt
\vspace*{\fill}
\begin{mdframed}
\begin{figure}[H]
	\vspace{-2em}
	\begin{mathpar}
		\boxed{\Gamma \vdash} \hspace{1.5em}
		$\inferrule[Empty]{ }{\emptyset \vdash}$ \hspace{1.5em}
		$\inferrule[Extend]{\Gamma \vdash \\ 0\Gamma \vdash S :_0 \mathcal{U}}{\Gamma, x :_q S \vdash}$
	\end{mathpar}\\
	\begin{mathpar}
		\boxed{\Gamma \vdash M :_\sigma S} \hspace{1.5em}
		$\inferrule[Var]{0\Gamma_1, x :_\sigma S, 0\Gamma_2 \vdash}{0\Gamma_1, x :_\sigma S, 0\Gamma_2 \vdash x :_\sigma S}$ \hspace{1.5em}
		$\inferrule[Universe]{0\Gamma \vdash}{0\Gamma \vdash \mathcal{U} :_0 \mathcal{U}}$
	\end{mathpar}
	\begin{mathpar}
		$\inferrule[Conversion]{\Gamma \vdash M :_\sigma S \\ 0\Gamma \vdash S :_0 \mathcal{U} \\ 0\Gamma \vdash T :_0 \mathcal{U} \\ S \rightsquigarrow U \leftsquigarrow T}{\Gamma \vdash M :_\sigma T}$
	\end{mathpar}
	\begin{mathpar}
		$\inferrule[$\to$-Formation]{0\Gamma \vdash S :_0 \mathcal{U} \\ 0\Gamma, x :_0 S \vdash T :_0 \mathcal{U}}{0\Gamma \vdash (x :_q S) \to T :_0 \mathcal{U}}$ \hspace{1.5em}
		$\inferrule[$\to$-Intro]{\Gamma, x :_{\sigma q} S \vdash M :_\sigma T}{\Gamma \vdash (\lambda x :_q S.\ M) :_\sigma (x :_q S) \to T}$
	\end{mathpar}
	\begin{mathpar}
		$\inferrule[$\to$-Elim$_1$]{\Gamma_1 \vdash M :_\sigma (x :_q S) \to T \\ \Gamma_2 \vdash N :_1 S}{\Gamma_1 + \sigma q \Gamma_2 \vdash M\ N :_\sigma T}$ \hspace{1.5em}
		$\inferrule[$\to$-Elim$_0$]{\Gamma \vdash M :_\sigma (x :_q S) \to T \\\\ \sigma q = 0 \\ 0\Gamma \vdash N :_0 S}{\Gamma \vdash M\ N :_\sigma T}$
	\end{mathpar}
	\begin{mathpar}
		$\inferrule[$\otimes$-Formation]{0\Gamma \vdash S :_0 \mathcal{U} \\ 0\Gamma, x :_0 S \vdash T :_0 \mathcal{U}}{0\Gamma \vdash (x :_q S) \otimes T :_0 \mathcal{U}}$ \hspace{1.5em}
		$\inferrule[$\otimes$-Intro$_1$]{\Gamma_1 \vdash M :_1 S \\ \Gamma_2 \vdash N :_\sigma T[M/x]}{\sigma q \Gamma_1 + \Gamma_2 \vdash (M, N) :_\sigma (x :_q S) \otimes T}$ 
	\end{mathpar}
	\begin{mathpar}
		$\inferrule[$\otimes$-Intro$_0$]{\sigma q = 0 \\ 0\Gamma \vdash M :_0 S \\ \Gamma \vdash N :_\sigma T[M/x]}{\Gamma \vdash (M, N) :_\sigma (x :_q S) \otimes T}$
	\end{mathpar}
	\begin{mathpar}
		$\inferrule[$\otimes$-Elim]{\Gamma_1 \vdash M :_\sigma (x :_q S) \otimes T \\ 0\Gamma_1, z :_0 (x :_q S) \otimes T \vdash U :_0 \mathcal{U} \\ \Gamma_2, x :_{\sigma q} S, y :_\sigma T \vdash N :_\sigma U[(x, y)/z]}{\Gamma_1 + \Gamma_2 \vdash \acode{let}_{(x\,:_q\,S) \otimes T}\ z@(x, y) = M\ \acode{in}_U\ N :_\sigma U[M/z]}$
	\end{mathpar}
	\begin{mathpar}
		$\inferrule[$\mathbb{1}$-Formation]{0\Gamma \vdash}{0\Gamma \vdash \mathbb{1} :_0 \mathcal{U}}$ \hspace{1.5em}
		$\inferrule[$\mathbb{1}$-Intro]{0\Gamma \vdash}{0\Gamma \vdash () :_\sigma \mathbb{1}}$ \hspace{1.5em}
		$\inferrule[$\mathbb{1}$-Elim]{\Gamma_1 \vdash M :_\sigma \mathbb{1} \\\\ 0\Gamma_1, x :_0 \mathbb{1} \vdash S :_0 \mathcal{U} \\ \Gamma_2 \vdash N :_\sigma S[()/x]}{\Gamma_1 + \Gamma_2 \vdash \acode{let}_\mathbb{1}\ x@() = M\ \acode{in}_S\ N :_\sigma S[M/x]}$
	\end{mathpar}
	\caption{Quantitative type theory as described by \cite{svoboda_additive_2021} and inspired by \cite{atkey_syntax_2018}. Features a single type universe $\mathcal{U}$. $\rightsquigarrow$ denotes reduction. Quantities $q \in \{0, 1, \omega\}$ satisfy $p + q = q + p$, $0 + q = q$, $\omega + q = \omega$, $1 + 1 = \omega$, $p q = q p$, $0q = 0$, $1q = q$ and $\omega \omega = \omega$.}
	\label{fig:qtt}
\end{figure}
\end{mdframed}
\vspace*{\fill}
\clearpage

\section{Uniqueness Type Theory}\label{sec:uniqueness}
\cite{smetsers_guaranteeing_1994} and \cite{barendsen_uniqueness_1996} introduce a type system for the Clean programming language with the goal of guaranteeing referential uniqueness to enable many of the applications described in \cref{sec:ltt}. The core idea is to use the linear and non-linear types of linear type systems, but instead of keeping them entirely separate or allowing for a coercion from non-linear types to linear types, the coercion is inverted, allowing for the conversion of linear types to non-linear types. 

The motivation for this idea is that both properly linear type systems that guarantee referential uniqueness through careful library design and invariably unique type systems are too restrictive: For some use-cases of referential uniqueness, like destructive updates, it is perfectly acceptable to discard the uniqueness guarantee at some point, because the lack of a static uniqueness guarantee can still be mitigated through other means, as for example in \cref{sec:beans}, or because it is simply not needed for the remainder of the program. 

As long as there is a coercion from linear types to non-linear types, uniqueness type systems refer to linear types as ``unique'' (tagged with $*$ in the type system) and non-linear types as ``non-unique'' or ``shared'' (tagged with $!$ in the type system). The temporal guarantee becomes inverted: While linear types ensure that a variable is always used exactly once in the future, uniqueness types ensure that a variable has always been used exactly once in the past \citep{de_vries_making_2009}\citep{sergey_linearity_2022}.
\begin{mathpar}
	$\inferrule[Shallow-Cast-Left]{\Gamma, x :\ !\tau \vdash e : \tau'}{\Gamma, x : *\tau \vdash e : \tau'}$ \hspace{1.5em}
	$\inferrule[Shallow-Cast-Right]{\Gamma \vdash e : *\tau}{\Gamma \vdash e :\ !\tau}$
\end{mathpar}

\subsubsection{Uniqueness types by library design}
Implementations like the one in Linear Haskell \citep{bernardy_linear_2018} acknowledge the over-restrictiveness of linear type theory and make the coercion from linear to non-linear types a part of their library design as well: If MArray represents an array that is always guaranteed to be linear because its constructor has type $\mathrm{newMArray} :\ !\mathbb{N} \multimap (\mathrm{MArray}\ \alpha \multimap\ !\beta) \multimap\ !\beta$, then the coercion is $\mathrm{freeze} : \mathrm{MArray}\ \alpha \multimap\ !(\mathrm{Array}\ \alpha)$. 

Note that since this coercion requires switching out the entire type of the array, coercing nested arrays in constant time becomes problematic. A recent apporach by \cite{spiwack_linearly_2022} attempts to mitigate this issue by introducing a system of linear capabilities on top of the linear type system, so that Read and Read+Write capabilities are managed and passed around implicitly, freeze removes the Write capability from an MArray and gaining linear read access to a nested array within requires the Write capability for the outer array. We will go into some more detail on this approach in \cref{sec:borrowingbackground}.

\subsubsection{Lambda calculus and uniqueness types}
Unfortunately, the original type system of Clean was formulated in terms of graph rewriting and not lambda calculus, which meant that advances by the rest of the type theory research community were difficult to transfer over to uniqueness typing, a deficit that was only resolved much later by \cite{de_vries_making_2009}. In his thesis, de Vries first provides a uniqueness type system based on lambda calculus resembling that of Clean and then iteratively refines it with the goal of introducing higher-rank polymorphism \citep{peyton_jones_practical_2007}.

\subsubsection{Challenges}
In uniqueness type systems, there are a number of challenges that do not appear in linear type systems. 

\paragraph{Unique types within shared types} The first is that types are inherently less composable. In properly linear type theory, non-linear and linear types can be mixed freely, as long as the provided resources are present to create them. For example, $!(\tau_1 \otimes \tau_2)$ is just a non-linear product, whereas $(!\tau_1) \otimes \tau_2$ is a linear product where the first type is non-linear. 

If, on the other hand, the type system is invariably unique and $\tau_1$ is linear, then the former example $!(\tau_1 \otimes \tau_2)$ is malformed, as deconstructing the product and obtaining the linear $\tau_1$ will not actually yield us a guarantee that the corresponding value has not been shared in the past, as e.g.\ the product could have been shared. 

Hence, we must enforce that non-linear or non-unique types cannot contain linear or unique types. In invariably unique type systems, e.g.\ Wadler's steadfast linear types from \cite{wadler_is_1991}, the answer is usually to enforce this invariant when constructing a value. For example, constructing $!(\tau_1 \otimes \tau_2)$ becomes possible only when both $\tau_1$ and $\tau_2$ are non-linear.

With uniqueness types, the situation is unfortunately more complicated: If it is possible to discard the uniqueness of a value, then the invariant that non-unique types cannot contain unique types does not just need to be enforced at construction, but after discarding the uniqueness of a value as well. 

Clean in particular resolves the issue by enforcing the invariant both at construction and at deconstruction; if the type of a value is malformed, then deconstructing it is not allowed. Note that an alternative solution would be that deconstructing it is allowed, but the contained values will again be non-unique.

\paragraph{Higher-order functions} Second, both in uniqueness type systems and invariably unique type systems, there is a question of what to do about function closures. When forming $\lambda x.\ e :\ !(\tau_1 \multimap \tau_2)$, the closure of the function is data that is dragged around by the resulting function. As such, the same considerations as for types $\tau_1 \otimes \tau_2$ apply, namely that values of non-linear type (the function) cannot be allowed to contain values of linear types (anything implicitly contained in the function closure), lest we could duplicate the function value and with that its closure. 

In an invariably unique type system without \textsc{Promotion} and \textsc{Dereliction}, this is somewhat easy to resolve: For example, Wadler's steadfast types require that every type in the closure of a function must be non-linear when forming the abstraction.

In a uniqueness type system, this situation is again much more complicated because unique functions can lose their uniqueness. But unlike types $\tau_1 \otimes \tau_2$, we cannot simply resolve it by checking whether a non-unique function contains a unique type in its closure after the uniqueness guarantee has been discarded, as the uniqueness of the elements in the closure is not part of the function type. Resolving this is a difficult challenge and we will delay the discussion of possible solutions to \cref{sec:designspace}.

\paragraph{Uniqueness is not a quantity} Finally, it is worth pointing out that uniqueness types cannot simply be added as a quantity to existing quantitative type theories. 

The trick described at the end of \cref{sec:qtt} that allows for specifying whether a value is linear or non-linear only on binders rests on the fact that there is a coercion from non-linear to linear values, so that a function $f : \tau_1 \multimap\ !\tau_2$ can be coerced to $f : \tau_1 \multimap \tau_2$, after which the amount of required $\tau_1$ is scaled up by the amount of $\tau_2$ resources needed in the context. In other words, uniqueness types are not ``quantitative'' in the sense that the intuitions for reasoning about amounts of resources do not apply to them.

Instead, one approach to combine uniqueness types with dependent type theory is Graded Modal Dependent Type Theory (GRTT) \citep{moon_graded_2021}, a dependent type theory with a linearly typed substructural base that allows attaching generic modalities to types. \cite{sergey_linearity_2022} integrate uniqueness types into Granule \citep{orchard_quantitative_2019}, a non-dependently-typed precursor to GRTT. Unfortunately, their type system does not allow the use of unique types within other unique types, only within linear types. The uniqueness fragment of the type system can be found in \cref{fig:marshall-unique}.
\clearpage
\topskip0pt
\vspace*{\fill}
\begin{mdframed}
	\vspace{-2em}
	\begin{figure}[H]
		\begin{mathpar}
			$\inferrule[Var]{ }{!\Gamma, x : \tau \vdash x : \tau}$ \hspace{1.5em}
			$\inferrule[$\multimap$-Intro]{\Gamma, x : \tau_1 \vdash e : \tau_2}{\Gamma \vdash \lambda x.\ e : \tau_1 \multimap \tau_2}$ \hspace{1.5em}
			$\inferrule[$\multimap$-Elim]{\Gamma_1, e_1 : \tau_1 \multimap \tau_2 \\ \Gamma_2 \vdash e_2 : \tau_1}{\Gamma_1, \Gamma_2 \vdash e_1\ e_2 : \tau_2}$
		\end{mathpar}
		\begin{mathpar}
			$\inferrule[Contract]{\Gamma, x :\ !\tau', x :\ !\tau' \vdash e : \tau}{\Gamma, x :\ !\tau' \vdash e : \tau}$ \hspace{1.5em}
			$\inferrule[$\mathbb{1}$-Intro]{ }{!\Gamma \vdash () : \mathbb{1}}$ \hspace{1.5em}
			$\inferrule[$\mathbb{1}$-Elim]{\Gamma_1 \vdash e_1 : \mathbb{1} \\ \Gamma_2 \vdash e_2 : \tau}{\Gamma_1, \Gamma_2 \vdash \acode{let}\ () = e_1\ \acode{in}\ e_2 : \tau}$
		\end{mathpar}
		\begin{mathpar}
			$\inferrule[$\otimes$-Intro]{\Gamma_1 \vdash e_1 : \tau_1 \\ \Gamma_2 \vdash e_2 : \tau_2}{\Gamma_1, \Gamma_2 \vdash (e_1, e_2) : \tau_1 \otimes \tau_2}$ \hspace{1.5em}
			$\inferrule[$\otimes$-Elim]{\Gamma_1 \vdash e_1 : \tau_1 \otimes \tau_2 \\ \Gamma_2, x : \tau_1, y : \tau_2 \vdash e_2 : \tau_3}{\Gamma_1, \Gamma_2 \vdash \acode{let}\ (x, y) = e_1\ \acode{in}\ e_2 : \tau_3}$
		\end{mathpar}
		\begin{mathpar}
			$\inferrule[Dereliction]{\Gamma, x : \tau_1 \vdash e : \tau_2}{\Gamma, x :\ !\tau_1 \vdash e : \tau_2}$ \hspace{1.5em}
			$\inferrule[Promotion]{!\Gamma \vdash e : \tau}{!\Gamma \vdash\ !e :\ !\tau}$ \hspace{1.5em}
			$\inferrule[$!$-Elim]{\Gamma_1 \vdash e_1 :\ !\tau_1 \\ \Gamma_2, x :\ !\tau_1 \vdash e_2 : \tau_2}{\Gamma_1, \Gamma_2 \vdash \acode{let}\ !x = e_1\ \acode{in}\ e_2 : \tau_2}$
		\end{mathpar}
		\begin{mathpar}
			$\inferrule[Shallow-Cast]{\Gamma \vdash e : *\tau}{\Gamma \vdash \&e :\ !\tau}$ \hspace{1.5em}
			$\inferrule[Copy]{\Gamma_1 \vdash e_1 :\ !\tau_1 \\ \Gamma_2, x : *\tau_1 \vdash e_2 :\ !\tau_2}{\Gamma_1, \Gamma_2 \vdash \acode{copy}\ e_1\ \acode{as}\ x\ \acode{in}\ e_2 :\ !\tau_2}$ \hspace{1.5em}
			$\inferrule[Necessitation]{\emptyset \vdash e : \tau}{!\Gamma \vdash *e : *\tau}$
		\end{mathpar}
		\caption{Properly linear type system of \cite{sergey_linearity_2022} that includes a uniqueness modality. Unique values cannot be contained within unique values, as \textsc{Shallow-Cast} would allow duplicating the inner unique values. As a result, all the built-in container types are linear. \textsc{Copy} allows copying unique values; the non-linearity of $\tau_2$ makes $*$ act as a relative monad \citep{altenkirch_monads_2015} over $!$. \textsc{Weaken} is integrated within \textsc{Var}, \textsc{$\mathbb{1}$-Intro} and \textsc{Necessitation}.}
		\label{fig:marshall-unique}
	\end{figure}
\end{mdframed}
\vspace*{\fill}
\clearpage

\section{Borrowing}\label{sec:borrowingbackground}
Linear type theory, quantitative type theory and uniqueness type theory all have one inconvenience in common: Every function consumes all of its arguments, and so a function that only reads from a linear or unique argument will lose the reference in the process. As we are working in the context of pure functional programming languages, a reference being ``read-only'' in a function refers to the fact that the reference does not escape the function.

In most basic formulations of linear and uniqueness type theory, this is resolved by adjusting the encoding of linear and unique functions to manually thread the read-only reference through the program: The function $f : \tau_1 \multimap \tau_2 \multimap \tau_3$, where the second argument does not escape in $\tau_3$, becomes $f : \tau_1 \multimap \tau_2 \multimap \tau_2 \otimes \tau_3$. Unfortunately, while this encoding can always be used, it is inconvenient and affects both the type-level encoding and the term-level usage of most functions. 

In order to alleviate this issue, many linear type theories implement a notion of ``borrowing'', i.e\. the non-consumption of arguments of which all components are guaranteed not to escape. 

\subsubsection{Implementations}
There are essentially three classes of implementations of borrowing:
\begin{enumerate}
	\item Applying syntactical restrictions on the types that can be borrowed \citep{wadler_linear_1990}
	\item Integrating an escape analysis with the type theory \citep{goos_observers_1992}\citep{kobayashi_quasi-linear_1999}\citep{goos_another_2002}
	\item Using type-level mechanisms to hide the explicit threading \citep{spiwack_linearly_2022}
\end{enumerate}

\cite{wadler_linear_1990} proposes the notion of a strict \lstinline|let! (x) y = u in v| expression to borrow a variable \verb|x| in \verb|u|, where \verb|u| is evaluated strictly and none of the components of the type of \verb|x| occur in the type of \verb|u|. If these conditions are fulfilled, then the type system guarantees that \verb|x| cannot be contained in the result of \verb|u|. Unfortunately, this is already quite restrictive, and when polymorphic- or erased types come into play, borrowing very rarely works when one wants it to work.

\cite{goos_observers_1992} implements so-called ``observer types'', which witness that the variable corresponding to the observer type has been borrowed. Then, upon leaving the scope of the borrow, the implementation checks whether the type of the resulting expression contains any observer types to ensure that none of the borrowed variables escape. \cite{goos_another_2002} and \cite{kobayashi_quasi-linear_1999} implement similar ideas.

\cite{spiwack_linearly_2022} use a system of linear constraints akin to type classes that can be consumed and returned implicitly by functions in order to remove the explicit threading from the term language of Linear Haskell. These constraints need to be freed manually and explicitly by the user, much like regular Linear Haskell types. Types subject to these constraints are tagged with an additional variable in order to connect constraints with regular types, and when returning a fresh constraint, an existential quantifier must be used to summon a variable to tag the constraint with. There are constraints for hiding the continuation-passing style when creating functions, constraints for reading and writing arrays, as well as constraints for slices of arrays.

We feel that constraint systems are an interesting approach, but that as formulated by \cite{spiwack_linearly_2022}, there is unfortunately still a large amount of syntactical overhead in the term language, as constraints have to be unpacked explicitly.

\subsubsection{Complex borrowing}
Finally, linear references may not just get lost when passed as function arguments themselves, but when stored within a function argument as well. For example, in $\mathrm{fst} : \tau_1 \otimes \tau_2 \multimap \tau_1$, $\tau_1$ escapes, but $\tau_2$ is lost. Unfortunately, this issue is much more difficult to resolve, as we cannot retain our unique reference to $\tau_2$ without also retaining our unique reference to $\tau_1 \otimes \tau_2$, with which we will also retain our unique reference to $\tau_1$, a reference that is not unique anymore after application of $\mathrm{fst}$. 

Inspired by Rust \citep{weiss_oxide_2021}, \cite{spiwack_linearly_2022} suggest introducing primitive ``lending'' functions that grant temporary read access into a type, for example as follows for a linear type \lstinline|MArray| that is allowed to contain other linear types:\\
\begin{code}
lend : MArray α ⊸ !ℕ ⊸ (!α ⊸ β) ⊸ Array α ⊗ β
\end{code}

Here, the unique reference to the array is consumed by \lstinline|lend| and hence unavailable in the continuation of type \lstinline|!α ⊸ β|. \lstinline|!α| provides read-only access to the array value indexed by the argument of type \lstinline|!ℕ|. The constraint mechanism additionally ensures that the argument of type \lstinline|!α| cannot escape in \lstinline|β|, as well as that the explicit threading of \lstinline|Array α| becomes implicit.

For \lstinline|MArray|, \cite{spiwack_linearly_2022} define another complex borrowing mechanism inspired by Rust: A function \lstinline|split| allows slicing an array \lstinline|a| at an index \lstinline|i|, consuming the original array in the process and producing two linear slices \lstinline|x := a[0:i)| and \lstinline|y := a[i:len(a))|, plus a token which witnesses that \lstinline|x| and \lstinline|y| are slices of \lstinline|a|. Then, a function \lstinline|join| can efficiently put two such slices with an associated witness token back together. Using continuations, we could also again define a lending primitive similar to the following and separately ensure that both arguments of type \lstinline|!(Array α)| do not escape in \lstinline|β|:\\
\begin{code}
lendSlices : Array α ⊸ !ℕ ⊸ (!(Array α) ⊸ !(Array α) ⊸ β) 
  ⊸ Array α ⊗ β
\end{code}

Primitive dedicated borrowing operations can be defined for many other types as well. We will not touch on this form of borrowing in this thesis and will instead focus on the simpler notion of borrowing function arguments themselves.

\subsubsection{Updating values within values}
In \cref{sec:beans}, we saw that performing destructive updates on values within containers can be problematic for Lean's destructive update mechanism as well. So far, we have only discussed approaches that grant read access to values within containers, but what about write access?

It turns out that granting write access to single elements is rather straight-forward and can be implemented in a similar manner as we did in \cref{sec:beans}. For arrays and a linear type \lstinline|α|, we could define a primitive function \lstinline|swap : Array α ⊸ !ℕ ⊸ α ⊸ Array α ⊗ α| that swaps out an element in the array with another and will hence retain the linearity of both. Using this primitive, we can define \lstinline|update : Array α ⊸ α ⊸ !ℕ ⊸ (α ⊸ α) ⊸ Array α|, where the second argument is a default value that is swapped into the array while the function of type \lstinline|(α ⊸ α)| is applied. On the other hand, implementing an efficient slicing primitive would be more difficult, as there is no guarantee that two sub-arrays can be efficiently rejoined without a token witnessing this. The Lean 4 destructive update mechanism struggles with this issue, too.