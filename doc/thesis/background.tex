\chapter{Background}\label{sec:background}

\section{Lean 4 Theorem Prover}\label{sec:lean4}
Lean 4 is a proof assistant and programming language developed primarily by Leonardo de Moura at Microsoft Research and Sebastian Ullrich at KIT with numerous open source contributions by other authors.

Up to and including version 3, Lean served only as a proof assistant, i.e. an interactive tool where users can input proofs that are then checked by the proof assistant. Many proof assistants also implement proof-generating automation, so-called ``tactic languages'', to make the task of writing a perfectly formal proof by hand less tedious. In Lean 3, the previous version of Lean, the same term language was used for proofs, theorem statements, definitions, programs, type declarations, specifications, and implementing automation. We will go into some detail on how Lean uses a single unified language for all of these things in \cref{sec:dtt}. For automation, the term language was also evaluated by a separate interpreter for more efficient execution.

Unfortunately, evaluating the term language using a separate interpreter would still yield inadequate performance, both for implementing more demanding automation and for implementing real world programs, which meant that such demanding programs were written in C++ instead and then made available to Lean using a foreign-function interface (FFI) \citep{ullrich_counting_2020}.

To improve on this, Lean 4 now implements its own self-hosted compiler toolchain for both a C backend and a work-in-progress LLVM backend, including its own IR, optimization pipeline and custom garbage collection algorithm. Being almost entirely self-hosted, Lean 4 is now also well capable of being used as a general purpose programming language.

Let us now consider some basic examples of Lean 4 code to get a feeling for the language. It should be noted that all of the following can be expressed more succinctly, but that we have chosen not to do so in order to make these snippets more easy to grasp for readers not familiar with Lean.
\begin{code}
def List.map (f : α → β) : List α → List β
  | []      => []
  | x :: xs => f x :: map f xs
\end{code}
\lstinline|List.map| is implemented by recursion on the second argument. The empty list again yields the empty list. If the list is a cons cell, we map the head of the cons cell using \lstinline|f|, recurse on the tail and then build a new cons cell from the results of both.
\begin{code}
def List.get? : List α → Nat → Option α
  | [],      _     => Option.none
  | x :: _,  0     => Option.some x
  | _ :: xs, n + 1 => List.get? xs n
\end{code}
\lstinline|List.get?| is implemented by recursion both on the provided list and the index provided in the second argument. It yields an \lstinline|Option α|, i.e. either \lstinline|Option.some α| if the index is in the list, or \lstinline|Option.none| otherwise.
\begin{code}
def Array.groupBy (p : α → α → Ordering) (xs : Array α)
  : RBMap α (Array α) p := Id.run do
  let mut result : RBMap α (Array α) p := RBMap.empty
  for x in xs do
    let group := Option.getD (RBMap.find? result x) #[]
    result := RBMap.insert result x (Array.push group x)
  return result
\end{code}
\lstinline|Array.groupBy| is implemented using an imperative domain-specific language (DSL) based on do-notation \citep{ullrich_beyond_2022}. It takes a relation \lstinline|p| that yields an \lstinline|Ordering|, i.e. whether the first argument is greater than, smaller than or equal to the second argument, as well as the array to group the elements of. It returns a red-black map ordered by \lstinline|p|, where the keys are arbitrary representatives of the group and the values denote groups \lstinline|Array α| of \lstinline|p|-equivalent values.

In order to use do-notation, we need to run the code in a monad. Since we do not intend to accumulate any effects and only use do-notation for its imperative domain-specific language (DSL), we use the \lstinline|Id| monad, entering it using \lstinline|Id.run|. First, we initialize a mutable but empty red-black map, denoting our result. Then, we iterate over every element \lstinline|x| in the provided array and look for a group in the current result that is \lstinline|p|-equivalent to \lstinline|x|. If we find such a group, we store it in \lstinline|group|. Otherwise, we allocate a new group for elements \lstinline|p|-equivalent to \lstinline|x| using the call to \lstinline|Option.getD|, which returns the first element if it was equal to \lstinline|some x| and otherwise returns the second element if it was equal to \lstinline|none|. Then, we add \lstinline|x| itself to the group and re-insert it into our mutable \lstinline|result| map. At the end, we return the accumulated \lstinline|result| map.

This piece of imperative code is implemented as a DSL, i.e. the imperative code is translated to a functional equivalent.
One may reasonably wonder why one would ever use a purely functional language to then write imperative code, translate it back to purely functional code, only to then compile the result to imperative machine code. But since Lean 4 is also an interactive theorem prover, the answer is simple: Imperative programming can be convenient, but it is easier to use a purely functional programming language for all the domains of proof, specification and writing programs at once, since all the computational effects are already neatly packed away. In fact, if we were to write a proof about \lstinline|Array.groupBy|, the first thing we would likely do is fold away the syntactical imperative layer in order to uncover the purely functional term representing the program, without ever having to think about loop invariants or state.

One may also wonder whether code written in this imperative manner is about as efficient as real imperative code. Because of the mechanism described in \cref{sec:beans}, this is indeed the case if the code in question is written in such a way that every value is unique, which is usually the case for the kind of code that one would also write in an imperative language. Importantly, functional code will benefit from this as well, which means that we can write compositional, functional code that also updates values in-place instead of making new allocations in every single combinator. It must however be noted that our implementation of \lstinline|Array.groupBy| is actually an example of an imperative implementation that is unexpectedly inefficient. We will resolve this inefficiency in \cref{sec:beans} when it is instrumental to do so.

For most of this thesis, we will not treat Lean 4 as a theorem prover, but instead as a purely functional programming language that implements dependent type theory. For more details on Lean 4 as a programming language, we refer to the book Functional Programming in Lean by \cite{christiansen_functional_2023}.

\section{Dependent Type Theory}\label{sec:dtt}
As described in \cref{sec:lean4}, Lean uses a single language for programming and proving. It accomplishes this by implementing dependent type theory (DTT), a type theory powerful enough to declare mathematical objects, implement programs and write specifications and proofs for both. What follows is only a very brief introduction to some of the important details of Lean's type theory, and we recommend the book Theorem Proving in Lean 4 by \cite{avigad_theorem_2022} for a proper introduction.

The central idea of DTT is that types are allowed to depend on terms. In most type theories, terms and types are entirely different constructs, and while terms have types, terms cannot be used in types. Removing this restriction blurs the line between programs and their static specification.

There are several mechanisms required to make this work: 
\begin{enumerate}
	\item In a quantified type $\forall x.\ \tau(x)$, the variable $x$ is allowed to range over terms of a type (e.g. $x \triangleq n : \mathbb{N}$), not just types themselves as is the case in languages that support polymorphic functions $\forall \alpha.\ \tau(\alpha)$.
	\item Terms in types can be reduced with the usual reduction rules of lambda calculus, e.g. $(\forall x.\ \tau((\lambda y.\ y)\ x)) \equiv (\forall x.\ \tau(x))$.
	\item Support for inductive type families, which are essentially algebraic data types where each constructor creates a term in a type that can be parametrized by other terms. For example, we might declare a type $\lambda \alpha : \mathrm{Type}.\ \lambda n : \mathbb{N}.\ \mathrm{Vec}\ \alpha\ n$ for lists over a type $\alpha$ of size $n$ with constructors $\mathrm{nil} : \mathrm{Vec}\ \alpha\ 0$ and $\mathrm{cons} : \forall n : \mathbb{N}.\ \alpha \to \mathrm{Vec}\ \alpha\ n \to \mathrm{Vec}\ \alpha\ (n+1)$. Each inductive type family also yields a recursion principle that allows for pattern matching and using recursion on the value of a type to compute an accumulate value.
\end{enumerate}

In addition to $\forall x : \tau_1.\ \tau_2(x)$, dependent type theories also always support dependent product types $(x : \tau_1) \times \tau_2(x)$ and dependent sum types $(x : \tau_1) + \tau_2(x)$.

For convenience, Lean's type theory also supports a separate type universe of propositions $\mathbb{P}$, the terms of which are types $p : \mathbb{P}$ with proof terms $h : p$ witnessing the truth of the proposition $p$. For example, if $\mathrm{refl} : \forall x.\ x = x$, we have $(\mathrm{refl}\ n) : (n = n) : \mathbb{P}$ and $(\mathrm{refl}\ n) : (n + 1 - 1 = n) : \mathbb{P}$ for $n : \mathbb{N}$, as $n + 1 - 1 \equiv n$. Meanwhile, there is no term $h : (n = n + 1) : \mathbb{P}$. For less trivial propositions, we use the recursion principles of inductive type families, which become induction principles if the accumulated value is a proposition $p : \mathbb{P}$. 

What distinguishes propositions $p : \mathbb{P}$ from other types is that $\mathbb{P}$ is impredicative and proof-irrelevant, i.e. whenever we quantify over a proposition $p : \mathbb{P}$, the resulting type is again a proposition, and for proofs $h_1, h_2 : p$, we have $h_1 = h_2$. In other words, propositions are contained to $\mathbb{P}$ and all proofs of a proposition are considered equal, i.e. only their existence is relevant, not the concrete content of the proof. Hence, proofs are inherently non-computational; since the content of a proof is irrelevant, it can be erased. These two features allow Lean to introduce additional non-computational classical axioms into its universe of propositions $\mathbb{P}$, most prominently the axiom of choice.

Putting all of these mechanisms together yields a type theory powerful enough to declare types like $\mathrm{Vec}\ \alpha\ n$ and all the usual objects that are used in mathematics, as well as logical operators like $\cdot \land \cdot$, $\exists x.\ p(x)$, $\cdot = \cdot$ and even well-founded recursion. Additionally, the term language is strong enough to write arbitrary programs, as well as classical proofs within $\mathbb{P}$.

For a detailed formal description of Lean's type theory, we refer to \citep{carneiro_type_2019}.

\section{Intermediate Representations for Lean 4}\label{sec:irs}

\section{Counting Immutable Beans}\label{sec:beans}

\section{Linear Type Theory}\label{sec:ltt}
Type systems typically guarantee certain functional or extensional properties for a given program using a typing relation $\Gamma \vdash e : \tau$, where $\Gamma$ is a set of type judgements $x : \tau'$. The following \textsc{Exchange}, \textsc{Weaken} and \textsc{Contract} rules are usually assumed implicitly:
\begin{mathpar}
	$\inferrule[Exchange]{\Gamma_1, y : \tau_2, \Gamma_2, x : \tau_1, \Gamma_3 \vdash e : \tau}{\Gamma_1, x : \tau_1, \Gamma_2, y : \tau_2, \Gamma_3 \vdash e : \tau}$ \hspace{1.5em}
	$\inferrule[Weaken]{\Gamma \vdash e : \tau}{\Gamma, x : \tau' \vdash e : \tau}$ \hspace{1.5em}
	$\inferrule[Contract]{\Gamma, x : \tau', x : \tau' \vdash e : \tau}{\Gamma, x : \tau' \vdash e : \tau}$
\end{mathpar} 
In other words, judgements in the context can be reordered, discarded and duplicated freely.

In the formal descriptions of dependent type theories, like the one described in \cref{sec:dtt}, \textsc{Exchange} is inhibited, because $\tau_2$ may depend on $x$, which induces an order of declaration on variables in the context. But if we want to guarantee only extensional properties for $e$, \textsc{Weaken} and \textsc{Contract} can always be freely assumed, as there is nothing to gain from retaining the exact count of every judgement in the context. 

However, if we wish to guarantee non-functional or intensional properties for a given program, then discarding the \textsc{Weaken} and \textsc{Contract} rules can be useful. And indeed doing just that constitutes the core idea of substructural type theories: By retaining the exact number of each judgement in the context, we can use typing rules to count objects in our program to ensure various kinds of intensional properties, like for example information flows \citep{choudhury_dependent_2022}.

However, in most substructural type theories, the extra detail in the context is used to count variable uses. \cite{girard_linear_1987} was the first to notice that not assuming \textsc{Weaken} and \textsc{Contract} allows one to define so-called linear logics with dualities that allow reasoning about resource usage, and \cite{wadler_linear_1990} transports this idea to form a linear type theory with a set of entirely separate linear and non-linear types prefixed by $!$, even on the term level. In both, discarding \textsc{Weaken} and \textsc{Contract} is used to enforce the invariant that linear variables can only be used exactly once. \cite{wadler_is_1991} then goes on to define \textsc{Dereliction} and \textsc{Promotion} rules that allow coercing a non-linear type to a linear type, as well as a ``steadfast'' type system where there is no coercion between linear and non-linear types, but both use the same term language. 

With \textsc{Promotion} and \textsc{Dereliction}, linear types make a guarantee for the future: We do not know whether this variable has always been linear, but now that it is, we guarantee that it will be used exactly once. Meanwhile, in Wadler's ``steadfast'' version of the type system, as there is no coercion from non-linear to linear types, a linear variable has and will always be used exactly once. In a type system with \textsc{Promotion} and \textsc{Dereliction}, linearity is hence unsuited to guarantee the uniqueness of a variable, as it may have been duplicated in the past, an issue that is acknowledged by \cite{wadler_is_1991}. For linear type systems where linearity is guaranteed from construction onwards, \cite{chirimar_reference_1996} prove that the linearity of a variable implies the uniqueness of the associated reference.

Future linear type systems \citep{goos_observers_1992}\citep{atkey_syntax_2018}\citep{bernardy_linear_2018}\citep{brady_idris_2021}\citep{choudhury_graded_2021} \citep{li_linear_2022}\citep{spiwack_linearly_2022} always adopt one of these two approaches and either allow for a non-linear to linear coercion while attempting to guarantee linearity from the construction of a value onwards, or no coercion at all, thus cementing the idea of ``linearity'' referring to either ``always linear'' or ``linear from this point onwards''.

One common refinement of linear type theory is to allow the use of \textsc{Weaken}, but not \textsc{Contract}, specifically when the type theory only wants to guarantee that a reference is unique, but not that it is used. These type theories are also knows as ``affine'' \citep{tov_practical_2011}, though the term is often conflated with ``linear''.

Amongst others, the following types are commonly present in linear type theories:
\begin{itemize}
	\item $\tau_1 \multimap \tau_2$ for the type of linear functions that consume an argument of type $\tau_1$ on application
	\item $(!\tau_1) \multimap \tau_2$ for the type of non-linear functions that consume a non-linear argument of type $\tau_1$ on application and yield a $\tau_2$ that can be made non-linear through dereliction
	\item $\tau_1 \otimes \tau_2$ for the type of multiplicative linear products, where both arguments of type $\tau_1$ and $\tau_2$ are consumed on construction, and then re-obtained through pattern matching on the resulting value
	\item $\tau_1 \oplus \tau_2$ for the type of linear sums, where one of the arguments of type $\tau_1$ and $\tau_2$ is consumed depending on which constructor is used, and then re-obtained through pattern matching on the resulting value
\end{itemize}

\section{Quantitative Type Theory}
Quantitative type theory (QTT) applies the idea of linear type theory to dependent type theory. 

In linear type theories, the \textsc{App} rule is typically stated in a manner similar to the following in order to distribute the needed amount of judgements to both expressions:
\begin{mathpar}
	$\inferrule[App]{\Gamma_1 \vdash e_1 : \tau_1 \multimap \tau_2 \\ \Gamma_2 \vdash e_2 : \tau_1}{\Gamma_1, \Gamma_2 \vdash e_1\ e_2 : \tau_2}$
\end{mathpar}
When omitting the \textsc{Exchange} rule in dependent type theory, it is not clear that splitting the context into $\Gamma_1$ and $\Gamma_2$ is always possible, since types in $\Gamma_2$ may depend on variables in $\Gamma_1$. This seemingly technical issue induces a semantic problem as well: Which occurrences of a variable constitute a use? For example, what about occurrences in types?

After this issue was left unsolved for a long time, \cite{lindley_i_2016} resolved it by introducing a third kind of type to linear type theory, resulting in the three kinds of type ``linear'' (denoted as $1$), ``non-linear'' (denoted as $\omega$) and ``erased'' (denoted as $0$). ``erased'' specifies that a type or a term within DTT is not computationally relevant and will be erased by the compiler. Erased terms or types can only be used in other erased terms or types, and types are always erased. Finally, when a linear variable is used, it becomes erased, which justifies the use of $1$ to denote linear types and $0$ to denote erased types. \textsc{Exchange} is only omitted for variables of type $0$. In \textsc{App}, since we do not need to count the uses of variables of type $0$, we can freely distribute variables of type $1$ and $\omega$ to either $\Gamma_1$ or $\Gamma_2$, but duplicate all variables of type $0$ in their given order to $\Gamma_1$ and $\Gamma_2$. In other words, both the technical and the semantic issue are resolved at once. Quantitative type theory also retains the spirit of linear typing in that non-linear types can be coerced to linear types, which can subsequently be erased by using them in a non-erased term.

The implementations of QTT by \cite{lindley_i_2016} and \cite{atkey_syntax_2018} use an additional trick to make the description of the type theory more compact. As seen in \cref{sec:ltt}, while linear type theories usually denote linearity on the type $\tau$, quantitative type theory instead denotes it on the binder $:$ in $x : \tau$, which leads to $0$, $1$ and $\omega$ not simply being kinds of types, but ``quantities'' on the binders. Instead, there are only non-linear types, and the quantity on the binder determines the substructural restrictions on the variable. The context now instead contains quantified type judgements $x :[q] \tau$ for $q \in \{0, 1, \omega\}$ and the types described in \cref{sec:ltt} become the following dependent types:
\begin{itemize}
	\item $((x :[1]\ \tau_1) \to \tau_2) \triangleeq (\tau_1 \multimap \tau_2)$
	\item $((x :[1]\ \tau_1) \times (y :[1]\ \tau_2) \times \mathbb{1}) \triangleeq (\tau_1 \otimes \tau_2)$
	\item $((x :[1]\ \tau_1) + (y :[1]\ \tau_2) + \mathbb{0}) \triangleeq (\tau_1 \oplus \tau_2)$
\end{itemize}
Note that by denoting the linear quantity on the binder in $(x :[1]\ \tau_1) \to \tau_2$, we cannot specify a quantity for $\tau_2$ any more, as we could in linear type theory using $!$. But since we can always coerce $1$ to $\omega$, we can simply assume $\tau_2$ to be linear.

Finally, it is worth pointing out that both \cite{lindley_i_2016} and \cite{atkey_syntax_2018} specify a generic framework for adding additional quantities to the type theory, allowing, for example, the additional introduction of a $\leq1$ quantity representing affinity, or more accurate accounting of uses $n > 1$.

\section{Uniqueness Type Theory}\label{sec:uniqueness}

\section{Borrowing}\label{sec:borrowingbackground}

\section{Abstract Interpretation}